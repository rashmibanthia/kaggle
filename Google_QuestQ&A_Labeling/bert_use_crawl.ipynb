{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "bert_use_crawl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c0f48dc14bff42e29ad384ce3c8f8bb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c4320b3f63ea4621a6e4ae8fe11e280a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_86d39169382b4601ab9a01f6865aacf3",
              "IPY_MODEL_77c40757724a4aeaad67ce328101f1ec"
            ]
          }
        },
        "c4320b3f63ea4621a6e4ae8fe11e280a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86d39169382b4601ab9a01f6865aacf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f84c29581a774e9fb51a5272dcabaa6d",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_675619a44ddf43c3a75c1d161e741934"
          }
        },
        "77c40757724a4aeaad67ce328101f1ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a8280d43f124e3984b8fd2001d3f788",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 440M/440M [00:34&lt;00:00, 12.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94c6b761e5744ddfa29eb574d0f25bbd"
          }
        },
        "f84c29581a774e9fb51a5272dcabaa6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "675619a44ddf43c3a75c1d161e741934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a8280d43f124e3984b8fd2001d3f788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94c6b761e5744ddfa29eb574d0f25bbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4e8NoEg3QXd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "015a53ff-8e35-4eb1-edf8-fa059594012a"
      },
      "source": [
        "%tensorflow_version 2.0.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.0.0`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XrFPQbuX88O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8a93463e-61fb-43cf-b444-e931a449a0f2"
      },
      "source": [
        "import logging\n",
        "from os.path import join as path_join\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = 'gdrive/My Drive/kaggle/google-quest-challenge'\n",
        "code_dir = 'gdrive/My Drive/kaggle/google-quest-code/post_comp_code'\n",
        "logging.basicConfig(filename=path_join('gdrive','My Drive','kaggle','google-quest-code','post_comp_code','bertusecrawl_seed1977.log') ,level=logging.WARNING)\n",
        "usefeatures_path = path_join(data_dir,'4/') #path_join(data_dir,'use/') \n",
        "\n",
        "#data_dir = '/home/jupyter/google_quest_datasets/'\n",
        "#logging.basicConfig(filename=path_join('/home','jupyter','submissions','bert_use_crawl_seed_1977','bertusecrawl_seed1977.log') ,level=logging.WARNING)\n",
        "#usefeatures_path = path_join(data_dir,'use/')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8sG87oyX88T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "reqs = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'])\n",
        "installed_packages = [r.decode().split('==')[0] for r in reqs.split()]\n",
        "\n",
        "if 'sacremoses' not in installed_packages:\n",
        "   !pip install sacremoses >/dev/null\n",
        "\n",
        "if 'transformers' not in installed_packages:\n",
        "   !pip install transformers >/dev/null\n",
        "else:\n",
        "   import transformers\n",
        "   print(transformers.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFcQ5AR0X88X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "330b67d0-230c-4ede-e937-79de5f3bd0aa"
      },
      "source": [
        "import os\n",
        "import transformers, sys, os, gc\n",
        "import numpy as np, pandas as pd, math\n",
        "import torch, random, os, multiprocessing, glob\n",
        "import torch.nn.functional as F\n",
        "import torch, time\n",
        "\n",
        "#sys.path.insert(0, \"gdrive/My Drive/kaggle/google-quest-challenge/\")\n",
        "sys.path.insert(0, data_dir)\n",
        "\n",
        "from ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
        "from scipy.stats import spearmanr\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
        "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
        "    get_cosine_schedule_with_warmup, BertPreTrainedModel,\n",
        ")\n",
        "from os.path import join as path_join\n",
        "import html\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "#wiki/crawl\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "print(transformers.__version__)\n",
        "\n",
        "def set_seeds(SEED=42):\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "SEED=2019\n",
        "set_seeds(SEED)\n",
        "HEAD_TAIL = True\n",
        "\n",
        "train_csv_path = path_join(data_dir,'train.csv')\n",
        "test_csv_path = path_join(data_dir,'test.csv')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMFlarjwX88a",
        "colab_type": "text"
      },
      "source": [
        "# Get crawl embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0q4NSpnX88b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# added preprocessing from https://www.kaggle.com/wowfattie/3rd-place/data\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\n', '\\xa0', '\\t',\n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "mispell_dict = {\"aren't\" : \"are not\", \"aren  '  t\" : \"are not\",\n",
        "\"can't\" : \"cannot\", \"can  '  t\" : \"cannot\",\n",
        "\"couldn't\" : \"could not\",\"couldn  '  t\" : \"could not\",\n",
        "\"couldnt\" : \"could not\",\n",
        "\"didn't\" : \"did not\",\"didn  '  t\" : \"did not\",\n",
        "\"doesn't\" : \"does not\",\"doesn  '  t\" : \"does not\",\n",
        "\"doesnt\" : \"does not\", \n",
        "\"don't\" : \"do not\",\"don  '  t\" : \"do not\",\n",
        "\"hadn't\" : \"had not\",\"hadn  '  t\" : \"had not\",\n",
        "\"hasn't\" : \"has not\", \"hasn  '  t\" : \"has not\",\n",
        "\"haven't\" : \"have not\", \"haven  '  t\" : \"have not\",\n",
        "\"havent\" : \"have not\",  \n",
        "\"he'd\" : \"he would\", \"he  '  d\" : \"he would\",\n",
        "\"he'll\" : \"he will\",\"he  '  ll\" : \"he will\",\n",
        "\"he's\" : \"he is\",\"he  '  s\" : \"he is\",\n",
        "\"i'd\" : \"I would\",\"i  '  d\" : \"I would\",\n",
        "\"i'd\" : \"I had\",\"i  '  d\" : \"I had\",\n",
        "\"i'll\" : \"I will\",  \"i  '  ll\" : \"I will\",\n",
        "\"i'm\" : \"I am\",  \"i  '  m\" : \"I am\",\n",
        "\"isn't\" : \"is not\", \"isn  '  t\" : \"is not\",\n",
        "\"it's\" : \"it is\",  \"it  '  s\" : \"it is\",\n",
        "\"it'll\":\"it will\",   \"it  '  ll\":\"it will\",\n",
        "\"i've\" : \"I have\",   \"i  '  ve\" : \"I have\",\n",
        "\"let's\" : \"let us\",  \"let  '  s\" : \"let us\",\n",
        "\"mightn't\" : \"might not\",  \"mightn  '  t\" : \"might not\",\n",
        "\"mustn't\" : \"must not\",\"mustn  '  t\" : \"must not\",\n",
        "\"shan't\" : \"shall not\",\"shan  '  t\" : \"shall not\",\n",
        "\"she'd\" : \"she would\",\"she  '  d\" : \"she would\",\n",
        "\"she'll\" : \"she will\",\"she  '  ll\" : \"she will\",\n",
        "\"she's\" : \"she is\",\"she  '  s\" : \"she is\",\n",
        "\"shouldn't\" : \"should not\",\"shouldn  '  t\" : \"should not\",\n",
        "\"shouldnt\" : \"should not\",\n",
        "\"that's\" : \"that is\",\"that  '  s\" : \"that is\",\n",
        "\"thats\" : \"that is\",\n",
        "\"there's\" : \"there is\",\"there  '  s\" : \"there is\",\n",
        "\"theres\" : \"there is\",\n",
        "\"they'd\" : \"they would\",\"they  '  d\" : \"they would\",\n",
        "\"they'll\" : \"they will\",\"they  '  ll\" : \"they will\",\n",
        "\"they're\" : \"they are\",\"they  '  re\" : \"they are\",\n",
        "\"theyre\":  \"they are\",\n",
        "\"they've\" : \"they have\",\"they  '  ve\" : \"they have\",\n",
        "\"we'd\" : \"we would\",\"we  '  d\" : \"we would\",\n",
        "\"we're\" : \"we are\",\"we  '  re\" : \"we are\",\n",
        "\"weren't\" : \"were not\",\"weren  '  t\" : \"were not\",\n",
        "\"we've\" : \"we have\",\"we  '  ve\" : \"we have\",\n",
        "\"what'll\" : \"what will\",\"what  '  ll\" : \"what will\",\n",
        "\"what're\" : \"what are\",\"what  '  re\" : \"what are\",\n",
        "\"what's\" : \"what is\",\"what  '  s\" : \"what is\",\n",
        "\"what've\" : \"what have\",\"what  '  ve\" : \"what have\",\n",
        "\"where's\" : \"where is\",\"where  '  s\" : \"where is\",\n",
        "\"who'd\" : \"who would\",\"who  '  d\" : \"who would\",\n",
        "\"who'll\" : \"who will\",\"who  '  ll\" : \"who will\",\n",
        "\"who're\" : \"who are\",\"who  '  re\" : \"who are\",\n",
        "\"who's\" : \"who is\",\"who  '  s\" : \"who is\",\n",
        "\"who've\" : \"who have\",\"who  '  ve\" : \"who have\",\n",
        "\"won't\" : \"will not\",\"won  '  t\" : \"will not\",\n",
        "\"wouldn't\" : \"would not\",\"wouldn  '  t\" : \"would not\",\n",
        "\"you'd\" : \"you would\",\"you  '  d\" : \"you would\",\n",
        "\n",
        "\"you'll\" : \"you will\",\"you  '  ll\" : \"you will\",\n",
        "\"you're\" : \"you are\",\"you  '  re\" : \"you are\",\n",
        "\"you've\" : \"you have\",\"you  '  ve\" : \"you have\",\n",
        "\"'re\": \" are\",\n",
        "\"wasn't\": \"was not\",\"wasn  '  t\": \"was not\",\n",
        "\"we'll\":\" will\",\"we  '  ll\":\" will\",\n",
        "\"didn't\": \"did not\",\"didn  '  t\": \"did not\",\n",
        "\"tryin'\":\"trying\"}\n",
        "\n",
        "\n",
        "def clean_text(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "    return x\n",
        "\n",
        "\n",
        "def clean_numbers(x):\n",
        "    x = re.sub('[0-9]{5,}', '#####', x)\n",
        "    x = re.sub('[0-9]{4}', '####', x)\n",
        "    x = re.sub('[0-9]{3}', '###', x)\n",
        "    x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def _get_mispell(mispell_dict):\n",
        "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
        "    return mispell_dict, mispell_re\n",
        "\n",
        "\n",
        "def replace_typical_misspell(text):\n",
        "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
        "\n",
        "    def replace(match):\n",
        "        return mispellings[match.group(0)]\n",
        "\n",
        "    return mispellings_re.sub(replace, text)\n",
        "\n",
        "\n",
        "def clean_data(df, columns: list):\n",
        "    for col in columns:\n",
        "        df[col] = df[col].apply(lambda x: clean_numbers(x))\n",
        "        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n",
        "        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7rpd8lCX88e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_coefs(word,*arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "\n",
        "def load_embeddings(path):\n",
        "    with open(path,'rb') as f:\n",
        "        emb_arr = pickle.load(f)\n",
        "    return emb_arr\n",
        "\n",
        "\n",
        "def build_matrix(embedding_path: str = '',\n",
        "                 embedding_path_spellcheck: str = path_join(data_dir, 'wiki-news-300d-1M.vec'),\n",
        "                 word_dict: dict = None, max_features: int = 100000,\n",
        "                 embed_size: int= 300, ):\n",
        "\n",
        "    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8'))\n",
        "    embedding_index = load_embeddings(embedding_path)\n",
        "\n",
        "    nb_words = min(max_features, len(word_dict))\n",
        "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
        "    unknown_words = []\n",
        "    for word, i in word_dict.items():\n",
        "        key = word\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            continue\n",
        "        embedding_vector = embedding_index.get(word.lower())\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            continue\n",
        "        embedding_vector = embedding_index.get(word.upper())\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            continue\n",
        "        embedding_vector = embedding_index.get(word.capitalize())\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            continue\n",
        "        unknown_words.append(key)\n",
        "\n",
        "    print(f'{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings')\n",
        "    return embedding_matrix, nb_words, unknown_words\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvpvWcHgX88g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv(test_csv_path).fillna(' ')\n",
        "train = pd.read_csv(train_csv_path).fillna(' ')\n",
        "\n",
        "# train.question_body = train.question_body.apply(html.unescape)\n",
        "# train.answer        = train.answer.apply(html.unescape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3pnbqSBX88j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = clean_data(train, ['answer', 'question_body', 'question_title'])\n",
        "test = clean_data(test, ['answer', 'question_body', 'question_title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HYn8eg1X88m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "full_text = list(train['question_body']) + \\\n",
        "                       list(train['answer']) + \\\n",
        "                       list(train['question_title']) + \\\n",
        "                       list(test['question_body']) + \\\n",
        "                       list(test['answer']) + \\\n",
        "                       list(test['question_title'])\n",
        "tokenizer.fit_on_texts(full_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A7UALFzX88o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c20d69cd-df04-43a5-80a2-8a863091724b"
      },
      "source": [
        "embed_size=300\n",
        "embedding_path = path_join(data_dir,'crawl-300d-2M.pkl') #\"/kaggle/input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl\"\n",
        "\n",
        "embedding_matrix, nb_words, unknown_words = build_matrix(embedding_path,  path_join(data_dir,'wiki-news-300d-1M.vec') , tokenizer.word_index,\n",
        "                                              100000, embed_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25.7148% words are not in embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q00NYQNwX88r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pickle dump embedding matrix \n",
        "\n",
        "# with open(\"embedding_matrix.pickle\", \"wb\") as output_file:\n",
        "#       pickle.dump(embedding_matrix, output_file)\n",
        "\n",
        "# with open(\"embedding_matrix.pickle\", \"rb\") as input_file:\n",
        "#       embedding_matrix = pickle.load(input_file)\n",
        " \n",
        "\n",
        " #pickle dump embedding matrix \n",
        "\n",
        "with open(path_join(code_dir,\"embedding_matrix.pickle\"), \"wb\") as output_file:\n",
        "      pickle.dump(embedding_matrix, output_file)\n",
        "\n",
        "with open(path_join(code_dir,\"embedding_matrix.pickle\"), \"rb\") as input_file:\n",
        "      embedding_matrix = pickle.load(input_file)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VViiqC8X88t",
        "colab_type": "text"
      },
      "source": [
        "### Create USE Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7lJoVOxX88u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import gc\n",
        "import pickle  \n",
        "import random\n",
        "import keras\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "#import keras.backend as K\n",
        "\n",
        "#from keras.models import Model\n",
        "#from keras.layers import Dense, Input, Dropout, Lambda\n",
        "#from keras.optimizers import Adam\n",
        "#from keras.callbacks import Callback\n",
        "from scipy.stats import spearmanr, rankdata\n",
        "from os.path import join as path_join\n",
        "from numpy.random import seed\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import MultiTaskElasticNet\n",
        "import re\n",
        "\n",
        "seed(SEED)\n",
        "#tf.random.set_seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSxz4h9pX88w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d641f255-67f5-4764-92b5-715214c5ebac"
      },
      "source": [
        "train = pd.read_csv(path_join(data_dir, 'train.csv'))\n",
        "test = pd.read_csv(path_join(data_dir, 'test.csv'))\n",
        "# train.question_body = train.question_body.apply(html.unescape)\n",
        "# train.answer        = train.answer.apply(html.unescape)\n",
        "\n",
        "print(train.shape, test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6079, 41) (476, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apjq9TKmX88y",
        "colab_type": "text"
      },
      "source": [
        "## Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "646bpEGFX88z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp = pd.DataFrame([['unknown1', 'LIFE_ARTS'],['unknown2', 'CULTURE'],['unknown3', 'SCIENCE'],\n",
        "             ['unknown4', 'STACKOVERFLOW'],['unknown5', 'TECHNOLOGY'],['unknown6', 'LIFE_ARTS'],\n",
        "              ['unknown7', 'CULTURE'],['unknown8', 'SCIENCE'],['unknown9', 'STACKOVERFLOW'],\n",
        "              ['unknown10', 'TECHNOLOGY'],['unknown11', 'CULTURE'],['unknown12', 'SCIENCE'],\n",
        "              ['unknown13', 'STACKOVERFLOW'],['unknown14', 'TECHNOLOGY'],['unknown15', 'LIFE_ARTS'],\n",
        "              ['unknown16', 'LIFE_ARTS'],['unknown17', 'CULTURE'],['unknown18', 'SCIENCE'],\n",
        "             ['unknown19', 'STACKOVERFLOW'],['unknown20', 'TECHNOLOGY'],['unknown21', 'LIFE_ARTS'],\n",
        "              ['unknown22', 'CULTURE'],['unknown23', 'SCIENCE'],['unknown24', 'STACKOVERFLOW'],\n",
        "              ['unknown25', 'TECHNOLOGY'],['unknown26', 'CULTURE'],['unknown27', 'SCIENCE'],\n",
        "              ['unknown28', 'STACKOVERFLOW'],['unknown29', 'TECHNOLOGY'],['unknown30', 'LIFE_ARTS'] ,\n",
        "                    ['unknown31', 'LIFE_ARTS'],['unknown32', 'CULTURE'],['unknown33', 'SCIENCE'],\n",
        "             ['unknown34', 'STACKOVERFLOW'],['unknown35', 'TECHNOLOGY'],['unknown36', 'LIFE_ARTS'],\n",
        "              ['unknown37', 'CULTURE'],['unknown38', 'SCIENCE'],['unknown39', 'STACKOVERFLOW'],\n",
        "              ['unknown40', 'TECHNOLOGY']\n",
        "             ],columns =  ['netloc', 'category'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA2CWcT9X881",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef5817b2-2060-4adf-8a6d-9d2b38ddfd23"
      },
      "source": [
        "find = re.compile(r\"^[^.]*\")\n",
        "\n",
        "train['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
        "test['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
        "\n",
        "features = ['netloc', 'category']\n",
        "merged = pd.concat([train[features], test[features]])\n",
        "\n",
        "#Adding unknown url\n",
        "merged = merged.append(tmp)\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "ohe.fit(merged)\n",
        "\n",
        "features_train = ohe.transform(train[features]).toarray()\n",
        "features_test = ohe.transform(test[features]).toarray()\n",
        "print(features_train.shape, features_test.shape)\n",
        "#Need to add 40 more unknown categories for private test set !! "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6079, 104) (476, 104)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJBrcvYnX884",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = usefeatures_path\n",
        "embed = hub.load(module_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz9aiDeaX886",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_columns = ['question_title', 'question_body', 'answer']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_iLKnD2X889",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dac73ca9-42ce-4e46-919f-39e65d7a12ae"
      },
      "source": [
        "embeddings_train = {}\n",
        "embeddings_test = {}\n",
        "for text in input_columns:\n",
        "    print(text)\n",
        "    train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
        "    test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
        "    \n",
        "    curr_train_emb = []\n",
        "    curr_test_emb = []\n",
        "    batch_size = 4\n",
        "    ind = 0\n",
        "    while ind*batch_size < len(train_text):\n",
        "        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
        "        ind += 1\n",
        "        \n",
        "    ind = 0\n",
        "    while ind*batch_size < len(test_text):\n",
        "        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
        "        ind += 1    \n",
        "        \n",
        "    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n",
        "    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n",
        "    \n",
        "del embed\n",
        "gc.collect()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question_title\n",
            "question_body\n",
            "answer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63QnNuWYX88_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pickle dump embeddings_train #use features\n",
        "\n",
        "with open(path_join(code_dir,\"embeddings_train.pickle\"), \"wb\") as output_file:\n",
        "      pickle.dump(embeddings_train, output_file)\n",
        "\n",
        "with open(path_join(code_dir,\"embeddings_train.pickle\"), \"rb\") as input_file:\n",
        "      embeddings_train = pickle.load(input_file)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV148X0qX89B",
        "colab_type": "text"
      },
      "source": [
        "### Distance features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_5yypKeX89C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n",
        "\n",
        "cos_dist = lambda x, y: (x*y).sum(axis=1)\n",
        "\n",
        "dist_features_train = np.array([\n",
        "   l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
        "   l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
        "   l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n",
        "   cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
        "   cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
        "   cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n",
        "]).T\n",
        "\n",
        "dist_features_test = np.array([\n",
        "    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
        "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
        "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n",
        "    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
        "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
        "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n",
        "]).T\n",
        "\n",
        "X_train = np.hstack([item for k, item in embeddings_train.items()] + [features_train, dist_features_train])\n",
        "X_test = np.hstack([item for k, item in embeddings_test.items()] + [features_test, dist_features_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HXWDgV7X89E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pickle dump embeddings_train #use features\n",
        "\n",
        "with open(path_join(code_dir,\"X_train.pickle\"), \"wb\") as output_file:\n",
        "      pickle.dump(X_train, output_file)\n",
        "\n",
        "with open(path_join(code_dir,\"X_train.pickle\"), \"rb\") as input_file:\n",
        "      X_train = pickle.load(input_file)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW1CmTdNX89G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unique_hosts = list(set(train['host'].unique().tolist() + test['host'].unique().tolist()))\n",
        "# host_dict = {i + 1: e for i, e in enumerate(unique_hosts)}\n",
        "# host_dict_reverse = {v: k for k, v in host_dict.items()}\n",
        "\n",
        "# unique_categories = list(set(train['category'].unique().tolist() + test['category'].unique().tolist()))\n",
        "# category_dict = {i + 1: e for i, e in enumerate(unique_categories)}\n",
        "# category_dict_reverse = {v: k for k, v in category_dict.items()}\n",
        "max_len = 500\n",
        "max_len_title = 30\n",
        "train_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['question_body']), maxlen = max_len)\n",
        "train_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['answer']), maxlen = max_len)\n",
        "train_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['question_title']), maxlen = max_len_title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTbA_guyX89I",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGUsOO7oX89J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpatialDropout(nn.Dropout2d):\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
        "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
        "        x = x.squeeze(2)  # (N, T, K)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLgrsmtwX89K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Change the model to include only question+title embedding and answer+title embedding\n",
        "LEN_USE_FEATURES = 1646 #1606 \n",
        "class BertForSequenceClassification_v2Answer(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
        "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification (or regression if config.num_labels==1) loss.\n",
        "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
        "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config,embedding_matrix=None):\n",
        "        \n",
        "        #super(BertForSequenceClassification, self).__init__(config)\n",
        "        super(BertForSequenceClassification_v2Answer,self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.embedding = nn.Embedding(*embedding_matrix.shape) \n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.embedding_dropout = SpatialDropout(0.3)\n",
        "\n",
        "        self.linear_q_add = nn.Linear(300, 128)\n",
        "        self.linear_q_add1 = nn.Linear(128, self.config.num_labels)\n",
        "       \n",
        "        # MINE - works\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.linear1 = nn.Linear(config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels  ,config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels)\n",
        "        self.classifier = nn.Linear(config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels  , self.config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        usefeatures = None, \n",
        "        question_data = None,\n",
        "        answer_data = None,\n",
        "        title_data = None\n",
        "    ):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "       \n",
        "        last_hidden_state = outputs[0][:,0,:].reshape((-1, 1, 768)) #CLS Token \n",
        "        pooled_output = outputs[1] \n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "        # MINE - works\n",
        "        h12 = hidden_states[-1][:, 0].reshape((-1, 1, 768))\n",
        "        h11 = hidden_states[-2][:, 0].reshape((-1, 1, 768))\n",
        "        h10 = hidden_states[-3][:, 0].reshape((-1, 1, 768))\n",
        "        h9  = hidden_states[-4][:, 0].reshape((-1, 1, 768))\n",
        "        all_h = torch.cat([ h9, h10, h11, h12], 1)\n",
        "        all_h_mean = torch.mean(all_h, 1).reshape((-1,1,768))\n",
        "        mean_pool = torch.mean(torch.cat([last_hidden_state,all_h_mean],1),1)\n",
        "        pooled_output = self.dropout(mean_pool)\n",
        "\n",
        "\n",
        "        #Embedding \n",
        "        h_embedding_q = self.embedding(question_data)\n",
        "        h_embedding_q = self.embedding_dropout(h_embedding_q)\n",
        "\n",
        "        h_embedding_a = self.embedding(answer_data)\n",
        "        h_embedding_a = self.embedding_dropout(h_embedding_a)\n",
        "\n",
        "        h_embedding_t = self.embedding(title_data)\n",
        "        h_embedding_t = self.embedding_dropout(h_embedding_t)\n",
        "\n",
        "        add = torch.cat((h_embedding_t, h_embedding_q, h_embedding_a), 1) #h_embedding_a,\n",
        "        add = self.linear_q_add(torch.mean(add, 1))\n",
        "        add = self.linear_q_add1(add)\n",
        "\n",
        "        x = torch.nn.ELU()(self.linear1(torch.cat([pooled_output,usefeatures,add],1)))\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        #logits = self.classifier(pooled_output)\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpGPtrfcX89M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Change the model to include only question+title embedding and answer+title embedding\n",
        "LEN_USE_FEATURES = 1646 #1606 \n",
        "class BertForSequenceClassification_v2Question(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
        "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification (or regression if config.num_labels==1) loss.\n",
        "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
        "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config,embedding_matrix=None):\n",
        "        \n",
        "        #super(BertForSequenceClassification, self).__init__(config)\n",
        "        super(BertForSequenceClassification_v2Question,self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.embedding = nn.Embedding(*embedding_matrix.shape) \n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.embedding_dropout = SpatialDropout(0.3)\n",
        "\n",
        "        self.linear_q_add = nn.Linear(300, 128)\n",
        "        \n",
        "        self.linear_q_add1 = nn.Linear(128, self.config.num_labels)\n",
        "\n",
        "        \n",
        "        # MINE - works\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.linear1 = nn.Linear(config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels  ,config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels)\n",
        "        self.classifier = nn.Linear(config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels  , self.config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        usefeatures = None, \n",
        "        question_data = None,\n",
        "        answer_data = None,\n",
        "        title_data = None\n",
        "    ):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        last_hidden_state = outputs[0][:,0,:].reshape((-1, 1, 768)) #CLS Token \n",
        "        pooled_output = outputs[1] \n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "        # MINE - works\n",
        "        h12 = hidden_states[-1][:, 0].reshape((-1, 1, 768))\n",
        "        h11 = hidden_states[-2][:, 0].reshape((-1, 1, 768))\n",
        "        h10 = hidden_states[-3][:, 0].reshape((-1, 1, 768))\n",
        "        h9  = hidden_states[-4][:, 0].reshape((-1, 1, 768))\n",
        "        all_h = torch.cat([ h9, h10, h11, h12], 1)\n",
        "        all_h_mean = torch.mean(all_h, 1).reshape((-1,1,768))\n",
        "        mean_pool = torch.mean(torch.cat([last_hidden_state,all_h_mean],1),1)\n",
        "        pooled_output = self.dropout(mean_pool)\n",
        "\n",
        "\n",
        "        #Embedding \n",
        "        h_embedding_q = self.embedding(question_data)\n",
        "        h_embedding_q = self.embedding_dropout(h_embedding_q)\n",
        "\n",
        "        h_embedding_t = self.embedding(title_data)\n",
        "        h_embedding_t = self.embedding_dropout(h_embedding_t)\n",
        "\n",
        "        add = torch.cat((h_embedding_t,  h_embedding_q), 1) #h_embedding_a,\n",
        "        add = self.linear_q_add(torch.mean(add,1))\n",
        "        add = self.linear_q_add1(add)\n",
        "        \n",
        "        x = torch.nn.ELU()(self.linear1(torch.cat([pooled_output,usefeatures,add],1)))\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        \n",
        "        #logits = self.classifier(pooled_output)\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l6C7XEDX89O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train = pd.read_csv( path_join(data_dir,\"train.csv\"))\n",
        "test = pd.read_csv( path_join(data_dir, \"test.csv\"))\n",
        "# train.question_body = train.question_body.apply(html.unescape)\n",
        "# train.answer        = train.answer.apply(html.unescape)\n",
        "\n",
        "target_cols_questions = ['question_asker_intent_understanding', 'question_body_critical', \n",
        "               'question_conversational', 'question_expect_short_answer', \n",
        "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
        "               'question_interestingness_others', 'question_interestingness_self', \n",
        "               'question_multi_intent', 'question_not_really_a_question', \n",
        "               'question_opinion_seeking', 'question_type_choice',\n",
        "               'question_type_compare', 'question_type_consequence',\n",
        "               'question_type_definition', 'question_type_entity', \n",
        "               'question_type_instructions', 'question_type_procedure', \n",
        "               'question_type_reason_explanation', 'question_type_spelling', \n",
        "               'question_well_written'] \n",
        "\n",
        "target_cols_answers =  ['answer_helpful',\n",
        "               'answer_level_of_information', 'answer_plausible', \n",
        "               'answer_relevance', 'answer_satisfaction', \n",
        "               'answer_type_instructions', 'answer_type_procedure', \n",
        "               'answer_type_reason_explanation', 'answer_well_written']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM-5wTWpX89Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights_questions = np.ones((len(train),))\n",
        "mask = np.sum(train[target_cols_questions]>0.5,1)\n",
        "weights_questions = weights_questions + mask \n",
        "loss_weight_questions = 1.0 / weights_questions.mean()\n",
        "\n",
        "train['weights_questions'] = weights_questions\n",
        "target_cols_questions = target_cols_questions + ['weights_questions']\n",
        "\n",
        "\n",
        "weights_answers = np.ones((len(train),))\n",
        "mask = np.sum(train[target_cols_answers]>0.5,1)\n",
        "weights_answers = weights_answers + mask \n",
        "loss_weight_answers = 1.0 / weights_answers.mean()\n",
        "\n",
        "train['weights_answers'] = weights_answers\n",
        "target_cols_answers = target_cols_answers + ['weights_answers']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry38CKzhX89S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helpers\n",
        "# From the Ref Kernel's\n",
        "from math import floor, ceil\n",
        "\n",
        "def _get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    \n",
        "    if len(tokens) > max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "        \n",
        "    segments = []\n",
        "    first_sep = True\n",
        "    current_segment_id = 0\n",
        "    \n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            if first_sep:\n",
        "                first_sep = False \n",
        "            else:\n",
        "                current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    \n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return np.asarray(df[columns])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjI_DKVqX89V",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Z4DwHeX89W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def _trim_input_answers(title, question, answer, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239):\n",
        "    \n",
        "    #293+239+30 = 508 + 4 = 512\n",
        "    \n",
        "    t = tokenizer.tokenize(title)\n",
        "    q = tokenizer.tokenize(question)\n",
        "    a = tokenizer.tokenize(answer)\n",
        "    \n",
        "    t_len = len(t)\n",
        "    q_len = len(q)\n",
        "    a_len = len(a)\n",
        "\n",
        "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
        "        \n",
        "        if t_max_len > t_len:\n",
        "            t_new_len = t_len\n",
        "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
        "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
        "        else:\n",
        "            t_new_len = t_max_len\n",
        "      \n",
        "        if a_max_len > a_len:\n",
        "            a_new_len = a_len \n",
        "            q_new_len = q_max_len + (a_max_len - a_len)\n",
        "        elif q_max_len > q_len:\n",
        "            a_new_len = a_max_len + (q_max_len - q_len)\n",
        "            q_new_len = q_len\n",
        "        else:\n",
        "            a_new_len = a_max_len\n",
        "            q_new_len = q_max_len\n",
        "            \n",
        "            \n",
        "        # if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
        "        #     raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
        "        \n",
        "        # t = t[:t_new_len]\n",
        "        # q = q[:q_new_len]\n",
        "        # a = a[:a_new_len]\n",
        "\n",
        "\n",
        "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
        "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
        "        \n",
        "        q_len_head = round(q_new_len/2)\n",
        "        q_len_tail = -1* (q_new_len -q_len_head)\n",
        "        a_len_head = round(a_new_len/2)\n",
        "        a_len_tail = -1* (a_new_len -a_len_head)        ## Head+Tail method .\n",
        "        t = t[:t_new_len]\n",
        "        if HEAD_TAIL :\n",
        "            q = q[:q_len_head]+q[q_len_tail:]\n",
        "            a = a[:a_len_head]+a[a_len_tail:]\n",
        "        else:\n",
        "            q = q[:q_new_len]\n",
        "            a = a[:a_new_len] ## No Head+Tail ,usual processing\n",
        "    \n",
        "    return t, q, a\n",
        "\n",
        "\n",
        "\n",
        "def _convert_to_bert_inputs_answers(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"] \n",
        "\n",
        "    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
        "    input_masks = _get_masks(stoken, max_sequence_length)\n",
        "    input_segments = _get_segments(stoken, max_sequence_length)\n",
        "\n",
        "    return [input_ids, input_masks, input_segments]\n",
        "\n",
        "\n",
        "\n",
        "def compute_input_arays_answers(df, columns, tokenizer, max_sequence_length):\n",
        "    \n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for _, instance in tqdm(df[columns].iterrows(),position=0, leave=True ):\n",
        "        t, q, a  = instance.question_title,instance.question_body, instance.answer\n",
        "        t, q, a  = _trim_input_answers(t, q, a, tokenizer, max_sequence_length)\n",
        "        ids, masks, segments = _convert_to_bert_inputs_answers(t,q, a,tokenizer, max_sequence_length)\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "    return [\n",
        "        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
        "        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
        "        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNceLEsdX89Y",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8dyFP-cX89Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _trim_input(title, question, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=479):\n",
        "    \n",
        "    #3 + 479 + 30\n",
        "    \n",
        "    t = tokenizer.tokenize(title)\n",
        "    q = tokenizer.tokenize(question)\n",
        "    \n",
        "    t_len = len(t)\n",
        "    q_len = len(q)\n",
        "    \n",
        "    if (t_len+q_len+3) > max_sequence_length:\n",
        "        \n",
        "        if t_max_len > t_len:\n",
        "            t_new_len = t_len\n",
        "            q_max_len = q_max_len + (t_max_len - t_len)\n",
        "            q_new_len = q_max_len\n",
        "        else:\n",
        "            t_new_len = t_max_len\n",
        "            q_new_len = q_max_len\n",
        "      \n",
        "        if q_max_len > q_len:\n",
        "            q_new_len = q_len\n",
        "            \n",
        "            \n",
        "        if t_new_len+q_new_len+3 != max_sequence_length:\n",
        "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + q_new_len + 3)))\n",
        "        \n",
        "        t = t[:t_new_len]\n",
        "        q = q[:q_new_len]\n",
        "        \n",
        "    return t, q\n",
        "\n",
        "def _convert_to_bert_inputs(title, question, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] \n",
        "\n",
        "    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
        "    input_masks = _get_masks(stoken, max_sequence_length)\n",
        "    input_segments = _get_segments(stoken, max_sequence_length)\n",
        "\n",
        "    return [input_ids, input_masks, input_segments]\n",
        "\n",
        "def compute_input_arays(df, columns, tokenizer, max_sequence_length):\n",
        "    \n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for _, instance in tqdm(df[columns].iterrows(),position=0, leave=True ):\n",
        "        t, q  = instance.question_title, instance.question_body\n",
        "        t, q  = _trim_input(t, q, tokenizer, max_sequence_length)\n",
        "        ids, masks, segments = _convert_to_bert_inputs(t, q,tokenizer, max_sequence_length)\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "    return [\n",
        "        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
        "        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
        "        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
        "    ]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT5q6u6MX89b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, usefeatures,question_data, answer_data, title_data, lengths, labels = None):\n",
        "        \n",
        "        self.inputs = inputs\n",
        "        self.usefeatures = usefeatures\n",
        "        self.question_data = question_data\n",
        "        self.answer_data = answer_data\n",
        "        self.title_data = title_data\n",
        "        if labels is not None:\n",
        "            self.labels = labels\n",
        "        else:\n",
        "            self.labels = None\n",
        "        self.lengths = lengths\n",
        "       \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        input_ids       = self.inputs[0][idx]\n",
        "        input_masks     = self.inputs[1][idx]\n",
        "        input_segments  = self.inputs[2][idx]\n",
        "        lengths         = self.lengths[idx]\n",
        "        usef            = self.usefeatures[idx,:]\n",
        "        question_data   = self.question_data[idx,:]\n",
        "        answer_data     = self.answer_data[idx,:]\n",
        "        title_data      = self.title_data[idx,:]\n",
        "\n",
        "        if self.labels is not None: # targets\n",
        "            labels = self.labels[idx]\n",
        "            return input_ids, input_masks, input_segments,  usef,  question_data, answer_data, title_data, labels, lengths\n",
        "        return input_ids, input_masks, input_segments, usef,  question_data,answer_data, title_data,  lengths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTFuOOjgX89d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss_fn_c_answer(criterion, lgits, lbls):\n",
        "    return (criterion(lgits,lbls[:,0:9]) * lbls[:,9].view(-1,1)).mean()\n",
        "\n",
        "def get_loss_fn_c(criterion, lgits, lbls):\n",
        "    return (criterion(lgits,lbls[:,0:21]) * lbls[:,21].view(-1,1)).mean()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA4GDhh8X89f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_answers(train_loader, optimizer, criterion, scheduler):\n",
        "    \n",
        "    model_a.train()\n",
        "    avg_loss = 0.\n",
        "    tk0 = tqdm(enumerate(train_loader),position=0, leave=True )\n",
        "    \n",
        "    for idx, batch in tk0:\n",
        "        \n",
        "        input_ids, input_masks, input_segments, usefeatures, question_data, answer_data, title_data,labels, _ = batch\n",
        "        input_ids, input_masks, input_segments, usefeatures, question_data, answer_data, title_data,labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), usefeatures.to(device) ,question_data.to(device),answer_data.to(device),title_data.to(device),labels.to(device)            \n",
        "         \n",
        "        output_train = model_a(input_ids = input_ids.long(),\n",
        "                             labels = None,\n",
        "                             attention_mask = input_masks,\n",
        "                             token_type_ids = input_segments,\n",
        "                             usefeatures = usefeatures,\n",
        "                             question_data = question_data,\n",
        "                             answer_data = answer_data,\n",
        "                             title_data = title_data\n",
        "                            )\n",
        "        logits = output_train[0] #output preds\n",
        "\n",
        "        loss = get_loss_fn_c_answer(criterion,logits,labels)\n",
        "        #loss = get_loss_fn_b(criterion,logits,labels)\n",
        "        #loss = loss / accumulation_steps    #https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        avg_loss += loss.item() / len(train_loader)\n",
        "        del input_ids, input_masks, input_segments, labels, usefeatures, question_data, answer_data, title_data\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X15oxFEIX89h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_questions(train_loader, optimizer, criterion, scheduler):\n",
        "    \n",
        "    model_q.train()\n",
        "    avg_loss = 0.\n",
        "    tk0 = tqdm(enumerate(train_loader),position=0, leave=True )\n",
        "    \n",
        "    for idx, batch in tk0:\n",
        "        \n",
        "        input_ids, input_masks, input_segments, usefeatures, question_data, answer_data, title_data, labels, _ = batch\n",
        "        input_ids, input_masks, input_segments, usefeatures, question_data, answer_data, title_data, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device),usefeatures.to(device),question_data.to(device),answer_data.to(device),title_data.to(device), labels.to(device)            \n",
        "        output_train = model_q(input_ids = input_ids.long(),\n",
        "                             labels = None,\n",
        "                             attention_mask = input_masks,\n",
        "                             token_type_ids = input_segments,\n",
        "                             usefeatures = usefeatures,\n",
        "                             question_data = question_data,\n",
        "                             answer_data = answer_data,\n",
        "                             title_data = title_data\n",
        "                            )\n",
        "        logits = output_train[0] #output preds\n",
        "\n",
        "        loss = get_loss_fn_c(criterion,logits,labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        avg_loss += loss.item() / len(train_loader)\n",
        "        del input_ids, input_masks, input_segments, labels, usefeatures,  question_data, answer_data, title_data\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XONQZNUX89j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_model_answers(val_loader, val_shape, batch_size=8):  \n",
        "    \n",
        "    avg_val_loss = 0.\n",
        "    model_a.eval() # eval mode\n",
        "    \n",
        "    valid_preds = np.zeros((val_shape, 9))\n",
        "    original = np.zeros((val_shape, 9))\n",
        "    \n",
        "    tk0 = tqdm(enumerate(val_loader),position=0, leave=True )\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for idx, batch in tk0:\n",
        "            input_ids, input_masks, input_segments,usefeatures,  question_data, answer_data, title_data, labels, _ = batch\n",
        "            input_ids, input_masks, input_segments, usefeatures,  question_data, answer_data, title_data, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), usefeatures.to(device), question_data.to(device),answer_data.to(device),title_data.to(device),labels.to(device)            \n",
        "            \n",
        "            output_val = model_a(input_ids = input_ids.long(),\n",
        "                             labels = None,\n",
        "                             attention_mask = input_masks,\n",
        "                             token_type_ids = input_segments,\n",
        "                             usefeatures = usefeatures,\n",
        "                             question_data = question_data,\n",
        "                             answer_data = answer_data,\n",
        "                             title_data = title_data\n",
        "                            )\n",
        "            logits = output_val[0] #output preds\n",
        "            \n",
        "            vloss = get_loss_fn_c_answer(criterion,logits,labels)\n",
        "            avg_val_loss += vloss.item() / len(val_loader)\n",
        "\n",
        "\n",
        "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
        "            #original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
        "            original[idx*batch_size : (idx+1)*batch_size]    = labels[:,0:9].detach().cpu().squeeze().numpy()\n",
        "        \n",
        "        score = 0\n",
        "        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
        "\n",
        "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
        "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
        "        \n",
        "        for i in range(9):\n",
        "            print(i, spearmanr(original[:,i], preds[:,i]))\n",
        "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
        "    return avg_val_loss, score/9. , preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbOXTDVkX89l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_model_questions(val_loader, val_shape, batch_size=8):  \n",
        "    \n",
        "    avg_val_loss = 0.\n",
        "    model_q.eval() # eval mode\n",
        "    \n",
        "    valid_preds = np.zeros((val_shape, 21))\n",
        "    original = np.zeros((val_shape, 21))\n",
        "    \n",
        "    tk0 = tqdm(enumerate(val_loader),position=0, leave=True )\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for idx, batch in tk0:\n",
        "            input_ids, input_masks, input_segments, usefeatures, question_data, answer_data, title_data, labels, _ = batch\n",
        "            input_ids, input_masks, input_segments, usefeatures, question_data, answer_data, title_data, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device),usefeatures.to(device), question_data.to(device),answer_data.to(device),title_data.to(device),labels.to(device)            \n",
        "            \n",
        "            output_val = model_q(input_ids = input_ids.long(),\n",
        "                             labels = None,\n",
        "                             attention_mask = input_masks,\n",
        "                             token_type_ids = input_segments,\n",
        "                             usefeatures = usefeatures,\n",
        "                             question_data = question_data,\n",
        "                             answer_data = answer_data,\n",
        "                             title_data = title_data\n",
        "                            )\n",
        "            logits = output_val[0] #output preds\n",
        "            \n",
        "            vloss = get_loss_fn_c(criterion,logits,labels)\n",
        "          \n",
        "            avg_val_loss += vloss.item() / len(val_loader)\n",
        "\n",
        "\n",
        "            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
        "            #original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n",
        "            original[idx*batch_size : (idx+1)*batch_size]    = labels[:,0:21].detach().cpu().squeeze().numpy()\n",
        "        \n",
        "        score = 0\n",
        "        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
        "       \n",
        "        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n",
        "        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
        "        \n",
        "        for i in range(21):\n",
        "            print(i, spearmanr(original[:,i], preds[:,i]))\n",
        "            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n",
        "    return avg_val_loss, score/21, preds #return 21 questions columns\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh2HXawYX89n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "11e7aff6-c7fb-42a9-c077-7d3e5504ec75"
      },
      "source": [
        "\n",
        "set_seeds(SEED)\n",
        "\n",
        "question_tokenizer = BertTokenizer.from_pretrained(path_join(data_dir,'bert_en_uncased_L-12_H-768_A-12', 'assets','vocab.txt'))\n",
        "input_categories_q = list(train.columns[[1,2]]); \n",
        "\n",
        "bert_model_config_question = path_join(data_dir,'bert_config.json')\n",
        "bert_config_q = BertConfig.from_json_file(bert_model_config_question)\n",
        "bert_config_q.output_hidden_states = True\n",
        "bert_config_q.num_labels = 21\n",
        "\n",
        "answer_tokenizer = BertTokenizer.from_pretrained(path_join(data_dir,'bert_en_uncased_L-12_H-768_A-12', 'assets','vocab.txt'))\n",
        "input_categories_a = list(train.columns[[1,5,2]]); \n",
        "\n",
        "bert_model_config_answer = path_join(data_dir,'bert_config.json')\n",
        "bert_config_a = BertConfig.from_json_file(bert_model_config_answer)\n",
        "bert_config_a.output_hidden_states = True\n",
        "bert_config_a.num_labels = 9\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "NUM_FOLDS = 5  # change this\n",
        "BATCH_SIZE = 8\n",
        "epochs = 4     # change this\n",
        "ACCUM_STEPS = 1\n",
        "test_predictions = [] \n",
        "all_rhos = []\n",
        "\n",
        "kf = MultilabelStratifiedKFold(n_splits = NUM_FOLDS, random_state = SEED)\n",
        "\n",
        "y_train_q = train[target_cols_questions].values # dummy\n",
        "y_train_a = train[target_cols_answers].values # dummy\n",
        "\n",
        "print(bcolors.FAIL, f\"For Every Fold, Train {epochs} Epochs\", bcolors.ENDC)\n",
        "\n",
        "set_seeds(SEED)\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[91m For Every Fold, Train 4 Epochs \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4MtW2VNX89p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain_usefeatures = X_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxlmqSs-X89r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c0f48dc14bff42e29ad384ce3c8f8bb9",
            "c4320b3f63ea4621a6e4ae8fe11e280a",
            "86d39169382b4601ab9a01f6865aacf3",
            "77c40757724a4aeaad67ce328101f1ec",
            "f84c29581a774e9fb51a5272dcabaa6d",
            "675619a44ddf43c3a75c1d161e741934",
            "0a8280d43f124e3984b8fd2001d3f788",
            "94c6b761e5744ddfa29eb574d0f25bbd"
          ]
        },
        "outputId": "f84629ae-77ea-413f-d827-6a158fa1fb84"
      },
      "source": [
        "best_rhos = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train.values, y_train_q)):\n",
        "  if fold > 2:  \n",
        "    print(bcolors.HEADER, \"Current Fold:\", fold, bcolors.ENDC)\n",
        "    train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n",
        "    print(\"Train and Valid Shapes are\", train_df.shape, val_df.shape)   \n",
        "    print(bcolors.HEADER, \"Preparing train datasets....\", bcolors.ENDC)\n",
        "\n",
        "    #questions\n",
        "    inputs_train_q = compute_input_arays(train_df, input_categories_q, question_tokenizer, max_sequence_length=512)\n",
        "    outputs_train_q = compute_output_arrays(train_df, columns = target_cols_questions)\n",
        "    outputs_train_q = torch.tensor(outputs_train_q, dtype=torch.float32)\n",
        "    lengths_train_q = np.argmax(inputs_train_q[0] == 0, axis=1)\n",
        "    lengths_train_q[lengths_train_q == 0] = inputs_train_q[0].shape[1]\n",
        "\n",
        "    #answers\n",
        "    inputs_train_a = compute_input_arays_answers(train_df, input_categories_a, answer_tokenizer, max_sequence_length=512)\n",
        "    outputs_train_a = compute_output_arrays(train_df, columns = target_cols_answers)\n",
        "    outputs_train_a = torch.tensor(outputs_train_a, dtype=torch.float32)\n",
        "    lengths_train_a = np.argmax(inputs_train_a[0] == 0, axis=1)\n",
        "    lengths_train_a[lengths_train_a == 0] = inputs_train_a[0].shape[1]\n",
        "\n",
        "    print(bcolors.HEADER, \"Preparing Valid datasets....\", bcolors.ENDC)\n",
        "    #questions\n",
        "    inputs_valid_q = compute_input_arays(val_df, input_categories_q, question_tokenizer, max_sequence_length=512)\n",
        "    outputs_valid_q = compute_output_arrays(val_df, columns = target_cols_questions)\n",
        "    outputs_valid_q = torch.tensor(outputs_valid_q, dtype=torch.float32)\n",
        "    lengths_valid_q = np.argmax(inputs_valid_q[0] == 0, axis=1)\n",
        "    lengths_valid_q[lengths_valid_q == 0] = inputs_valid_q[0].shape[1]\n",
        "\n",
        "    #answers\n",
        "    inputs_valid_a = compute_input_arays_answers(val_df, input_categories_a, answer_tokenizer, max_sequence_length=512)\n",
        "    outputs_valid_a = compute_output_arrays(val_df, columns = target_cols_answers)\n",
        "    outputs_valid_a = torch.tensor(outputs_valid_a, dtype=torch.float32)\n",
        "    lengths_valid_a = np.argmax(inputs_valid_a[0] == 0, axis=1)\n",
        "    lengths_valid_a[lengths_valid_a == 0] = inputs_valid_a[0].shape[1]\n",
        "\n",
        "\n",
        "    print(bcolors.HEADER, \"Preparing Dataloaders Datasets....\", bcolors.ENDC)\n",
        "    #questions\n",
        "    train_set_q    = QuestDataset(inputs=inputs_train_q, \n",
        "                                  usefeatures=torch.FloatTensor(xtrain_usefeatures[train_index,:]), \n",
        "                                  question_data = torch.LongTensor(train_question_tokenized[train_index,:]),\n",
        "                                  answer_data = torch.LongTensor(train_answer_tokenized[train_index,:]),\n",
        "                                  title_data = torch.LongTensor(train_title_tokenized[train_index,:]),\n",
        "                                  lengths=lengths_train_q, labels=outputs_train_q)\n",
        "    train_loader_q = DataLoader(train_set_q, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    #answers\n",
        "    train_set_a    = QuestDataset(inputs=inputs_train_a,\n",
        "                                  usefeatures=torch.FloatTensor(xtrain_usefeatures[train_index,:]), \n",
        "                                  question_data = torch.LongTensor(train_question_tokenized[train_index,:]),\n",
        "                                  answer_data = torch.LongTensor(train_answer_tokenized[train_index,:]),\n",
        "                                  title_data = torch.LongTensor(train_title_tokenized[train_index,:]),\n",
        "                                  lengths=lengths_train_a, labels=outputs_train_a)\n",
        "    train_loader_a = DataLoader(train_set_a, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    #questions\n",
        "    valid_set_q    = QuestDataset(inputs=inputs_valid_q,\n",
        "                                  usefeatures=torch.FloatTensor(xtrain_usefeatures[val_index,:]), \n",
        "                                  question_data = torch.LongTensor(train_question_tokenized[val_index,:]),\n",
        "                                  answer_data = torch.LongTensor(train_answer_tokenized[val_index,:]),\n",
        "                                  title_data = torch.LongTensor(train_title_tokenized[val_index,:]),\n",
        "                                  lengths=lengths_valid_q, labels=outputs_valid_q)\n",
        "    valid_loader_q = DataLoader(valid_set_q, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "    #answers\n",
        "    valid_set_a    = QuestDataset(inputs=inputs_valid_a, \n",
        "                                  usefeatures=torch.FloatTensor(xtrain_usefeatures[val_index,:]),\n",
        "                                  question_data = torch.LongTensor(train_question_tokenized[val_index,:]),\n",
        "                                  answer_data = torch.LongTensor(train_answer_tokenized[val_index,:]),\n",
        "                                  title_data = torch.LongTensor(train_title_tokenized[val_index,:]),\n",
        "                                  lengths=lengths_valid_a, labels=outputs_valid_a)\n",
        "    valid_loader_a = DataLoader(valid_set_a, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "    i = 0\n",
        "    best_avg_loss_q,best_avg_loss_a    = 100.0 , 100.0\n",
        "    best_score_q, best_score_a      = -1. , -1.\n",
        "    best_param_loss_q, best_param_loss_a = None,None\n",
        "    best_param_score_q,best_param_score_a = None,None\n",
        "\n",
        "    #model for questions\n",
        "    model_q = BertForSequenceClassification_v2Question.from_pretrained('bert-base-uncased', config=bert_config_q, embedding_matrix = embedding_matrix);\n",
        "    model_q.zero_grad();\n",
        "    model_q.to(device);\n",
        "    torch.cuda.empty_cache()\n",
        "    model_q.train();\n",
        "\n",
        "    #model for answers\n",
        "    model_a = BertForSequenceClassification_v2Answer.from_pretrained('bert-base-uncased', config=bert_config_a, embedding_matrix = embedding_matrix);\n",
        "    model_a.zero_grad();\n",
        "    model_a.to(device);\n",
        "    torch.cuda.empty_cache()\n",
        "    model_a.train();\n",
        "\n",
        "    #questions\n",
        "    optimizer_q = torch.optim.AdamW(model_q.parameters(), lr=4e-5, eps=2e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss( reduction='none')\n",
        "    scheduler_q = get_cosine_schedule_with_warmup(optimizer_q, num_warmup_steps=0.01, num_training_steps= epochs*len(train_loader_q)//ACCUM_STEPS)\n",
        "\n",
        "    #answers\n",
        "    optimizer_a = torch.optim.AdamW(model_a.parameters(), lr=4e-5, eps=2e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss( reduction='none')\n",
        "    scheduler_a = get_cosine_schedule_with_warmup(optimizer_a, num_warmup_steps=0.01, num_training_steps= epochs*len(train_loader_a)//ACCUM_STEPS)\n",
        "\n",
        "    print(\"Training....\")\n",
        "\n",
        "    for epoch in tqdm(range(epochs),position=0, leave=True ):        \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        start_time_q   = time.time()\n",
        "        avg_loss_q    = train_model_questions(train_loader_q, optimizer_q, criterion, scheduler_q)\n",
        "        avg_val_loss_q, score_q, preds_q = val_model_questions(valid_loader_q, val_shape=val_df.shape[0])\n",
        "        elapsed_time_q = time.time() - start_time_q\n",
        "\n",
        "\n",
        "        start_time_a   = time.time()\n",
        "        avg_loss_a    = train_model_answers(train_loader_a, optimizer_a, criterion, scheduler_a)\n",
        "        avg_val_loss_a,  score_a, preds_a = val_model_answers(valid_loader_a, val_shape=val_df.shape[0])\n",
        "        elapsed_time_a = time.time() - start_time_q\n",
        "\n",
        "        p = np.hstack((preds_q,  preds_a)) #np.hstack((preds_q,  preds_a[:,21:30]))\n",
        "        o = np.hstack((outputs_valid_q[:,0:21].numpy(), outputs_valid_a[:,0:9].numpy()))   #outputs_valid_a[:,0:30].numpy()\n",
        "        score = np.mean([spearmanr(o[:, i], p[:,i]).correlation for i in range(30)])\n",
        "\n",
        "        print(bcolors.WARNING, 'Epoch {}/{} \\t Train loss Q={:.4f} \\t Val loss Q={:.4f} \\t  Score Q={:.5f} \\t Train loss A={:.4f} \\t Val loss A={:.4f} \\t Score A={:.5f}  \\t score={:.6f} \\t time={:.2f}s'.format(\n",
        "            epoch + 1, epochs, avg_loss_q, avg_val_loss_q,  score_q, avg_loss_a, avg_val_loss_a, score_a,  score, elapsed_time_a),\n",
        "        bcolors.ENDC\n",
        "        )\n",
        "\n",
        "        logging.warning('Epoch {}/{} \\t Train loss Q={:.4f} \\t Val loss Q={:.4f} \\t  Score Q={:.5f} \\t Train loss A={:.4f} \\t Val loss A={:.4f} \\t Score A={:.5f}  \\t score={:.6f} \\t time={:.2f}s'.format(\n",
        "            epoch + 1, epochs, avg_loss_q, avg_val_loss_q,  score_q, avg_loss_a, avg_val_loss_a, score_a,  score, elapsed_time_a))\n",
        "        \n",
        "        \n",
        "        all_rhos.append(score)\n",
        "\n",
        "        if best_score_q < score_q:\n",
        "            best_score_q = score_q\n",
        "            best_e_q = epoch\n",
        "            best_param_score_q = model_q.state_dict()\n",
        "        else:\n",
        "            i += 1\n",
        "            \n",
        "        if best_score_a < score_a:\n",
        "            best_score_a = score_a\n",
        "            best_e_a = epoch\n",
        "            best_param_score_a = model_a.state_dict()\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    torch.save(model_q.state_dict(),path_join(code_dir,'saved_models','Qmodel_save_fold_{}.pt'.format(fold)))\n",
        "    torch.save(model_a.state_dict(),path_join(code_dir,'saved_models','Amodel_save_fold_{}.pt'.format(fold)))\n",
        "    # torch.save(best_param_score_q,path_join('/home','jupyter', 'submissions','bert_use_crawl_seed_1977','saved_models_1977','Q_model_save_fold_{}.pt'.format(fold)))\n",
        "    # torch.save(best_param_score_a,path_join('/home','jupyter', 'submissions','bert_use_crawl_seed_1977','saved_models_1977','A_model_save_fold_{}.pt'.format(fold)))\n",
        "    best_rhos.append((best_score_q*21 + best_score_a*9)/30.0)\n",
        "    \n",
        "logging.warning(np.mean(best_rhos))\n",
        "print(\"Best Rhos:\",np.mean(best_rhos))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24it [00:00, 238.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[95m Current Fold: 3 \u001b[0m\n",
            "Train and Valid Shapes are (4868, 43) (1211, 43)\n",
            "\u001b[95m Preparing train datasets.... \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4868it [00:16, 288.09it/s]\n",
            "4868it [00:30, 161.20it/s]\n",
            "24it [00:00, 236.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[95m Preparing Valid datasets.... \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1211it [00:04, 295.11it/s]\n",
            "1211it [00:07, 162.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[95m Preparing Dataloaders Datasets.... \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0f48dc14bff42e29ad384ce3c8f8bb9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "609it [05:02,  2.30it/s]\n",
            "152it [00:23,  6.53it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.4112                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.3282875354306319, pvalue=8.015032325270209e-32)\n",
            "1 SpearmanrResult(correlation=0.587224731253727, pvalue=3.766975795886116e-113)\n",
            "2 SpearmanrResult(correlation=0.4192547116296415, pvalue=9.690015310448891e-53)\n",
            "3 SpearmanrResult(correlation=0.2868111402556978, pvalue=2.3066566311244492e-24)\n",
            "4 SpearmanrResult(correlation=0.29854771437908517, pvalue=2.3616354125669606e-26)\n",
            "5 SpearmanrResult(correlation=0.4080570662656732, pvalue=8.555548174967421e-50)\n",
            "6 SpearmanrResult(correlation=0.335472702938804, pvalue=3.0730262048973553e-33)\n",
            "7 SpearmanrResult(correlation=0.4962896545362192, pvalue=2.6860995997640515e-76)\n",
            "8 SpearmanrResult(correlation=0.5699199459936202, pvalue=3.1087576313244447e-105)\n",
            "9 SpearmanrResult(correlation=0.09670693985682288, pvalue=0.000752305619864957)\n",
            "10 SpearmanrResult(correlation=0.39651961891068643, pvalue=7.116805376669933e-47)\n",
            "11 SpearmanrResult(correlation=0.7266570519086335, pvalue=2.398705674683731e-199)\n",
            "12 SpearmanrResult(correlation=0.3434935799797687, pvalue=7.268704379986553e-35)\n",
            "13 SpearmanrResult(correlation=0.16065652532561933, pvalue=1.8926889630778338e-08)\n",
            "14 SpearmanrResult(correlation=0.33853259424721466, pvalue=7.462202688729943e-34)\n",
            "15 SpearmanrResult(correlation=0.47788470710590014, pvalue=4.1459439952452406e-70)\n",
            "16 SpearmanrResult(correlation=0.8096045092024435, pvalue=5.175524783149751e-282)\n",
            "17 SpearmanrResult(correlation=0.3337567045991255, pvalue=6.749393969810292e-33)\n",
            "18 SpearmanrResult(correlation=0.6915926736545595, pvalue=5.024325087839142e-173)\n",
            "19 SpearmanrResult(correlation=0.05351879259716705, pvalue=0.06262644857599128)\n",
            "20 SpearmanrResult(correlation=0.47643800054000174, pvalue=1.2252992452995203e-69)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "609it [05:01,  2.32it/s]\n",
            "152it [00:23,  6.54it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.36133                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.26341004742994223, pvalue=1.1398734881122547e-20)\n",
            "1 SpearmanrResult(correlation=0.4208001537442879, pvalue=3.7230311034454114e-53)\n",
            "2 SpearmanrResult(correlation=0.12985687405029397, pvalue=5.801814123182787e-06)\n",
            "3 SpearmanrResult(correlation=0.17648752659407632, pvalue=6.252736989511694e-10)\n",
            "4 SpearmanrResult(correlation=0.3142493889438712, pvalue=3.65880516082787e-29)\n",
            "5 SpearmanrResult(correlation=0.7649208488247073, pvalue=3.319725120945279e-233)\n",
            "6 SpearmanrResult(correlation=0.29681779026413235, pvalue=4.702974361660517e-26)\n",
            "7 SpearmanrResult(correlation=0.69619628984787, pvalue=2.9063346017734868e-176)\n",
            "8 SpearmanrResult(correlation=0.1892548758219261, pvalue=3.1528929299771025e-11)\n",
            "\u001b[93m Epoch 1/4 \t Train loss Q=3.4818 \t Val loss Q=3.3069 \t  Score Q=0.41120 \t Train loss A=2.9404 \t Val loss A=2.9174 \t Score A=0.36133  \t score=0.396241 \t time=651.79s \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "609it [05:01,  2.32it/s]\n",
            "152it [00:23,  6.54it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.42839                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.3637541295681284, pvalue=3.4416468489719496e-39)\n",
            "1 SpearmanrResult(correlation=0.6412738998061958, pvalue=3.049674214383349e-141)\n",
            "2 SpearmanrResult(correlation=0.42312930270983445, pvalue=8.72351828057186e-54)\n",
            "3 SpearmanrResult(correlation=0.292085013571161, pvalue=3.0227802510369665e-25)\n",
            "4 SpearmanrResult(correlation=0.3090452566947174, pvalue=3.263323707318914e-28)\n",
            "5 SpearmanrResult(correlation=0.42495867873122145, pvalue=2.7683979129197306e-54)\n",
            "6 SpearmanrResult(correlation=0.35981969204981357, pvalue=2.519892548351282e-38)\n",
            "7 SpearmanrResult(correlation=0.4980984666978462, pvalue=6.313728550498286e-77)\n",
            "8 SpearmanrResult(correlation=0.5975745689989291, pvalue=4.056907426730439e-118)\n",
            "9 SpearmanrResult(correlation=0.11347204369376601, pvalue=7.574713095938711e-05)\n",
            "10 SpearmanrResult(correlation=0.4288252902104562, pvalue=2.390588781257726e-55)\n",
            "11 SpearmanrResult(correlation=0.7349635444947871, pvalue=3.4145190572471235e-206)\n",
            "12 SpearmanrResult(correlation=0.37752979131386877, pvalue=2.586932524331495e-42)\n",
            "13 SpearmanrResult(correlation=0.1669580540835412, pvalue=5.062138608355057e-09)\n",
            "14 SpearmanrResult(correlation=0.3487884304402471, pvalue=5.775422347982817e-36)\n",
            "15 SpearmanrResult(correlation=0.49464606943805833, pvalue=9.936943882456561e-76)\n",
            "16 SpearmanrResult(correlation=0.8116722890390051, pvalue=1.399151814929559e-284)\n",
            "17 SpearmanrResult(correlation=0.33918259749434504, pvalue=5.5130788009237985e-34)\n",
            "18 SpearmanrResult(correlation=0.7038068436195385, pvalue=9.408602162943742e-182)\n",
            "19 SpearmanrResult(correlation=0.05177361457769421, pvalue=0.07169667228032693)\n",
            "20 SpearmanrResult(correlation=0.5148451086229867, pvalue=6.241322882604094e-83)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "609it [05:01,  2.32it/s]\n",
            "152it [00:23,  6.55it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.36439                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.2635742411136281, pvalue=1.0769143794191923e-20)\n",
            "1 SpearmanrResult(correlation=0.426890619937335, pvalue=8.17471236320521e-55)\n",
            "2 SpearmanrResult(correlation=0.16678944046814592, pvalue=5.2474329530827785e-09)\n",
            "3 SpearmanrResult(correlation=0.1961349756042202, pvalue=5.765904688038516e-12)\n",
            "4 SpearmanrResult(correlation=0.3119220941204074, pvalue=9.787591975753436e-29)\n",
            "5 SpearmanrResult(correlation=0.7623892851361519, pvalue=9.073534977053568e-231)\n",
            "6 SpearmanrResult(correlation=0.2847829495121013, pvalue=4.9822912833872504e-24)\n",
            "7 SpearmanrResult(correlation=0.6978755717997076, pvalue=1.8489097653698166e-177)\n",
            "8 SpearmanrResult(correlation=0.16917942181609524, pvalue=3.1416908186529542e-09)\n",
            "\u001b[93m Epoch 2/4 \t Train loss Q=3.1728 \t Val loss Q=3.2580 \t  Score Q=0.42839 \t Train loss A=2.7216 \t Val loss A=2.8742 \t Score A=0.36439  \t score=0.409191 \t time=651.41s \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "609it [05:02,  2.31it/s]\n",
            "152it [00:23,  6.53it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.43079                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.35612163330123847, pvalue=1.5960683926486257e-37)\n",
            "1 SpearmanrResult(correlation=0.6670070640579978, pvalue=1.0214400337179977e-156)\n",
            "2 SpearmanrResult(correlation=0.4218301277221167, pvalue=1.962566337694249e-53)\n",
            "3 SpearmanrResult(correlation=0.29161158329712716, pvalue=3.6341289194476814e-25)\n",
            "4 SpearmanrResult(correlation=0.31293975122853274, pvalue=6.372114919719014e-29)\n",
            "5 SpearmanrResult(correlation=0.4120509093671377, pvalue=7.840648895905089e-51)\n",
            "6 SpearmanrResult(correlation=0.35464690357329964, pvalue=3.3097318534245036e-37)\n",
            "7 SpearmanrResult(correlation=0.5048592440074648, pvalue=2.607165668993706e-79)\n",
            "8 SpearmanrResult(correlation=0.6048376596193177, pvalue=1.0287322474276726e-121)\n",
            "9 SpearmanrResult(correlation=0.1268580937112365, pvalue=9.51417434686202e-06)\n",
            "10 SpearmanrResult(correlation=0.425935428878058, pvalue=1.495634265070106e-54)\n",
            "11 SpearmanrResult(correlation=0.7355346225891809, pvalue=1.1300650873442129e-206)\n",
            "12 SpearmanrResult(correlation=0.3733209069095412, pvalue=2.417619837502668e-41)\n",
            "13 SpearmanrResult(correlation=0.17265778216859293, pvalue=1.4696523068959127e-09)\n",
            "14 SpearmanrResult(correlation=0.35070238661180336, pvalue=2.2844286802185517e-36)\n",
            "15 SpearmanrResult(correlation=0.4917248138676102, pvalue=9.986542001829498e-75)\n",
            "16 SpearmanrResult(correlation=0.809291770087877, pvalue=1.2578249343786363e-281)\n",
            "17 SpearmanrResult(correlation=0.34976371612804197, pvalue=3.603099989789866e-36)\n",
            "18 SpearmanrResult(correlation=0.7032597181649557, pvalue=2.3661489188236067e-181)\n",
            "19 SpearmanrResult(correlation=0.05026112696081775, pvalue=0.08040220367711325)\n",
            "20 SpearmanrResult(correlation=0.5314107998077561, pvalue=3.2822098559640874e-89)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "609it [05:01,  2.32it/s]\n",
            "152it [00:23,  6.54it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.35452                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.2236363657940095, pvalue=3.4260655350970075e-15)\n",
            "1 SpearmanrResult(correlation=0.4340963479324387, pvalue=8.053034570255379e-57)\n",
            "2 SpearmanrResult(correlation=0.15274685889348433, pvalue=9.223848558552571e-08)\n",
            "3 SpearmanrResult(correlation=0.1781132944394571, pvalue=4.325273171657267e-10)\n",
            "4 SpearmanrResult(correlation=0.30165203458389833, pvalue=6.780065972065438e-27)\n",
            "5 SpearmanrResult(correlation=0.7566600908470262, pvalue=2.2970454495734134e-225)\n",
            "6 SpearmanrResult(correlation=0.28034730204126457, pvalue=2.6260978876318318e-23)\n",
            "7 SpearmanrResult(correlation=0.6857362592525431, pvalue=5.405014909549077e-169)\n",
            "8 SpearmanrResult(correlation=0.17764908710594202, pvalue=4.806922430210533e-10)\n",
            "\u001b[93m Epoch 3/4 \t Train loss Q=3.0192 \t Val loss Q=3.2676 \t  Score Q=0.43079 \t Train loss A=2.5225 \t Val loss A=2.9329 \t Score A=0.35452  \t score=0.407909 \t time=651.66s \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "609it [05:01,  2.33it/s]\n",
            "152it [00:23,  6.54it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.43123                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.35622704095330215, pvalue=1.5147739936127277e-37)\n",
            "1 SpearmanrResult(correlation=0.6709526279548049, pvalue=3.148099621886673e-159)\n",
            "2 SpearmanrResult(correlation=0.4181555424455814, pvalue=1.9075349251004403e-52)\n",
            "3 SpearmanrResult(correlation=0.286750260290985, pvalue=2.36081543325426e-24)\n",
            "4 SpearmanrResult(correlation=0.3175829533890056, pvalue=8.801675435870202e-30)\n",
            "5 SpearmanrResult(correlation=0.40650917284861876, pvalue=2.1415399450705548e-49)\n",
            "6 SpearmanrResult(correlation=0.3537162790819126, pvalue=5.233706183975192e-37)\n",
            "7 SpearmanrResult(correlation=0.5067155187024671, pvalue=5.651303676266933e-80)\n",
            "8 SpearmanrResult(correlation=0.6003436332976951, pvalue=1.7706893810971772e-119)\n",
            "9 SpearmanrResult(correlation=0.1313320160801739, pvalue=4.530460112121955e-06)\n",
            "10 SpearmanrResult(correlation=0.42793593212404163, pvalue=4.211130247263417e-55)\n",
            "11 SpearmanrResult(correlation=0.733684097989317, pvalue=4.024498696741417e-205)\n",
            "12 SpearmanrResult(correlation=0.3720357053558337, pvalue=4.752457970091115e-41)\n",
            "13 SpearmanrResult(correlation=0.1751139162549217, pvalue=8.51409600943319e-10)\n",
            "14 SpearmanrResult(correlation=0.3511274956579255, pvalue=1.8575103628809406e-36)\n",
            "15 SpearmanrResult(correlation=0.49078026695938803, pvalue=2.095936282873219e-74)\n",
            "16 SpearmanrResult(correlation=0.8072133787674374, pvalue=4.4120523814872595e-279)\n",
            "17 SpearmanrResult(correlation=0.3539892923878781, pvalue=4.57608161377764e-37)\n",
            "18 SpearmanrResult(correlation=0.7052539623320672, pvalue=8.123109624067204e-183)\n",
            "19 SpearmanrResult(correlation=0.05456589950102518, pvalue=0.0576545528014978)\n",
            "20 SpearmanrResult(correlation=0.5359301042858966, pvalue=5.518866042140504e-91)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "609it [05:01,  2.32it/s]\n",
            "152it [00:23,  6.55it/s]\n",
            "100%|██████████| 4/4 [43:26<00:00, 651.52s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.35427                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.2353164175604493, pvalue=1.0659236096079346e-16)\n",
            "1 SpearmanrResult(correlation=0.4355091620877376, pvalue=3.2123544867990825e-57)\n",
            "2 SpearmanrResult(correlation=0.15840770137295557, pvalue=2.993270847703863e-08)\n",
            "3 SpearmanrResult(correlation=0.178979924871585, pvalue=3.5488116147730354e-10)\n",
            "4 SpearmanrResult(correlation=0.3034107390336779, pvalue=3.320790323724791e-27)\n",
            "5 SpearmanrResult(correlation=0.7519551047013322, pvalue=4.868214188943304e-221)\n",
            "6 SpearmanrResult(correlation=0.26616972267959355, pvalue=4.3633522283918445e-21)\n",
            "7 SpearmanrResult(correlation=0.6768376114925081, pvalue=4.779937460161976e-163)\n",
            "8 SpearmanrResult(correlation=0.18179872891954105, pvalue=1.8519944738536294e-10)\n",
            "\u001b[93m Epoch 4/4 \t Train loss Q=2.9247 \t Val loss Q=3.2815 \t  Score Q=0.43123 \t Train loss A=2.3872 \t Val loss A=2.9753 \t Score A=0.35427  \t score=0.408143 \t time=651.03s \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "25it [00:00, 248.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[95m Current Fold: 4 \u001b[0m\n",
            "Train and Valid Shapes are (4849, 43) (1230, 43)\n",
            "\u001b[95m Preparing train datasets.... \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4849it [00:18, 255.66it/s]\n",
            "4849it [00:30, 160.27it/s]\n",
            "19it [00:00, 189.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[95m Preparing Valid datasets.... \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1230it [00:04, 297.68it/s]\n",
            "1230it [00:07, 162.81it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[95m Preparing Dataloaders Datasets.... \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "607it [05:00,  2.61it/s]\n",
            "154it [00:23,  6.96it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.42418                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.36346781976122267, pvalue=1.0284062883890895e-39)\n",
            "1 SpearmanrResult(correlation=0.5896465792541354, pvalue=4.6089756767709597e-116)\n",
            "2 SpearmanrResult(correlation=0.4321725637517184, pvalue=3.891068796715643e-57)\n",
            "3 SpearmanrResult(correlation=0.2895347389890356, pvalue=3.5073062858505873e-25)\n",
            "4 SpearmanrResult(correlation=0.40163671547938906, pvalue=6.954237080715889e-49)\n",
            "5 SpearmanrResult(correlation=0.39936234957316225, pvalue=2.6502360511768924e-48)\n",
            "6 SpearmanrResult(correlation=0.34426560648101445, pvalue=1.5086350088989436e-35)\n",
            "7 SpearmanrResult(correlation=0.47650192572293454, pvalue=1.00356427582587e-70)\n",
            "8 SpearmanrResult(correlation=0.5872757379645263, pvalue=6.361245704914613e-115)\n",
            "9 SpearmanrResult(correlation=0.12091668704854462, pvalue=2.1186008425133875e-05)\n",
            "10 SpearmanrResult(correlation=0.5024155599800021, pvalue=1.2035503940207827e-79)\n",
            "11 SpearmanrResult(correlation=0.7340085325626564, pvalue=1.3723853192830704e-208)\n",
            "12 SpearmanrResult(correlation=0.33318719819373926, pvalue=2.840209475966927e-33)\n",
            "13 SpearmanrResult(correlation=0.17402164920883859, pvalue=8.051675934131912e-10)\n",
            "14 SpearmanrResult(correlation=0.35999094500960416, pvalue=6.137789246690034e-39)\n",
            "15 SpearmanrResult(correlation=0.4403057489644344, pvalue=1.7581009248481277e-59)\n",
            "16 SpearmanrResult(correlation=0.779353681971171, pvalue=1.432501477621529e-251)\n",
            "17 SpearmanrResult(correlation=0.3508010863392279, pvalue=6.205700497884171e-37)\n",
            "18 SpearmanrResult(correlation=0.6587839514196322, pvalue=5.876038202557523e-154)\n",
            "19 SpearmanrResult(correlation=0.04273807865195399, pvalue=0.13412211309390226)\n",
            "20 SpearmanrResult(correlation=0.5273870757244442, pvalue=5.313741438314558e-89)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "607it [05:00,  2.64it/s]\n",
            "154it [00:23,  6.96it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.3483                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.22338484638359066, pvalue=2.2483173186864696e-15)\n",
            "1 SpearmanrResult(correlation=0.4208064485116671, pvalue=5.777604650293615e-54)\n",
            "2 SpearmanrResult(correlation=0.1436182260747668, pvalue=4.2366081085423516e-07)\n",
            "3 SpearmanrResult(correlation=0.1763711882375013, pvalue=4.719074532860813e-10)\n",
            "4 SpearmanrResult(correlation=0.28414013573615854, pvalue=2.8327930378509478e-24)\n",
            "5 SpearmanrResult(correlation=0.7615544040114642, pvalue=1.487684919256781e-233)\n",
            "6 SpearmanrResult(correlation=0.3057505086291558, pvalue=4.980253469478538e-28)\n",
            "7 SpearmanrResult(correlation=0.6427072397541438, pvalue=2.8810639886268054e-144)\n",
            "8 SpearmanrResult(correlation=0.17639817261372207, pvalue=4.690008301441059e-10)\n",
            "\u001b[93m Epoch 1/4 \t Train loss Q=3.4757 \t Val loss Q=3.3012 \t  Score Q=0.42418 \t Train loss A=2.9381 \t Val loss A=2.8688 \t Score A=0.34830  \t score=0.401417 \t time=650.06s \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "607it [05:01,  2.64it/s]\n",
            "154it [00:23,  6.97it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.43901                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.37492656339351166, pvalue=2.4347189744892056e-42)\n",
            "1 SpearmanrResult(correlation=0.6412638043091994, pvalue=2.0008566694166668e-143)\n",
            "2 SpearmanrResult(correlation=0.4403436666348284, pvalue=1.7138026869211467e-59)\n",
            "3 SpearmanrResult(correlation=0.28531046570208374, pvalue=1.807481863873921e-24)\n",
            "4 SpearmanrResult(correlation=0.40793125137707104, pvalue=1.623367568311666e-50)\n",
            "5 SpearmanrResult(correlation=0.40902613125946247, pvalue=8.374892593346349e-51)\n",
            "6 SpearmanrResult(correlation=0.3622139084558106, pvalue=1.9637102178056986e-39)\n",
            "7 SpearmanrResult(correlation=0.521531757615168, pvalue=9.75872319757188e-87)\n",
            "8 SpearmanrResult(correlation=0.6160272675535635, pvalue=2.065405491431809e-129)\n",
            "9 SpearmanrResult(correlation=0.1311030569824642, pvalue=3.964941178848413e-06)\n",
            "10 SpearmanrResult(correlation=0.5216812581666646, pvalue=8.553366309255639e-87)\n",
            "11 SpearmanrResult(correlation=0.7479225416511596, pvalue=8.58265314561174e-221)\n",
            "12 SpearmanrResult(correlation=0.3529102713119735, pvalue=2.1802678440398036e-37)\n",
            "13 SpearmanrResult(correlation=0.17934310766821618, pvalue=2.3758924265031613e-10)\n",
            "14 SpearmanrResult(correlation=0.36371078392217027, pvalue=9.069609780477365e-40)\n",
            "15 SpearmanrResult(correlation=0.44675219509405206, pvalue=2.1902045584792773e-61)\n",
            "16 SpearmanrResult(correlation=0.782722453865084, pvalue=3.597090162361606e-255)\n",
            "17 SpearmanrResult(correlation=0.36112209003284007, pvalue=3.440780946782022e-39)\n",
            "18 SpearmanrResult(correlation=0.6596655017439889, pvalue=1.660991954478811e-154)\n",
            "19 SpearmanrResult(correlation=0.06330682000506588, pvalue=0.026403438091641914)\n",
            "20 SpearmanrResult(correlation=0.5503029297249105, pvalue=2.6578464108720723e-98)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "607it [05:01,  2.63it/s]\n",
            "154it [00:23,  6.97it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.36131                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.23216079815835228, pvalue=1.6263006160646712e-16)\n",
            "1 SpearmanrResult(correlation=0.40754030980597716, pvalue=2.0549009910463438e-50)\n",
            "2 SpearmanrResult(correlation=0.1800689333582243, pvalue=2.0057069370081132e-10)\n",
            "3 SpearmanrResult(correlation=0.20524369387904493, pvalue=3.6385514862730145e-13)\n",
            "4 SpearmanrResult(correlation=0.3075893770269847, pvalue=2.3052630229259003e-28)\n",
            "5 SpearmanrResult(correlation=0.7582707222346469, pvalue=2.1085162214510967e-230)\n",
            "6 SpearmanrResult(correlation=0.31610193907924444, pvalue=6.064210809044508e-30)\n",
            "7 SpearmanrResult(correlation=0.6582896651489644, pvalue=1.1910745115889432e-153)\n",
            "8 SpearmanrResult(correlation=0.1865329992520409, pvalue=4.302627008763718e-11)\n",
            "\u001b[93m Epoch 2/4 \t Train loss Q=3.1758 \t Val loss Q=3.2380 \t  Score Q=0.43901 \t Train loss A=2.7228 \t Val loss A=2.8694 \t Score A=0.36131  \t score=0.415697 \t time=651.09s \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "607it [05:01,  2.62it/s]\n",
            "154it [00:23,  6.96it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.43877                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.3775112161846803, pvalue=6.017297858661358e-43)\n",
            "1 SpearmanrResult(correlation=0.6466427102448019, pvalue=1.3853031536203497e-146)\n",
            "2 SpearmanrResult(correlation=0.4365672979746514, pvalue=2.1423107866063324e-58)\n",
            "3 SpearmanrResult(correlation=0.28260553760328916, pvalue=5.089663709620854e-24)\n",
            "4 SpearmanrResult(correlation=0.41459531356715024, pvalue=2.7811662578573103e-52)\n",
            "5 SpearmanrResult(correlation=0.40052137398714344, pvalue=1.3420012100438178e-48)\n",
            "6 SpearmanrResult(correlation=0.36305962605115094, pvalue=1.2698418675079247e-39)\n",
            "7 SpearmanrResult(correlation=0.5217423316921789, pvalue=8.104735093540628e-87)\n",
            "8 SpearmanrResult(correlation=0.6268302796587187, pvalue=2.980068384448997e-135)\n",
            "9 SpearmanrResult(correlation=0.12794838713099024, pvalue=6.7547571678674476e-06)\n",
            "10 SpearmanrResult(correlation=0.5121329888325064, pvalue=3.405860719921573e-83)\n",
            "11 SpearmanrResult(correlation=0.7494828410929935, pvalue=3.2732189265010756e-222)\n",
            "12 SpearmanrResult(correlation=0.35429931309303003, pvalue=1.0900412269457415e-37)\n",
            "13 SpearmanrResult(correlation=0.1734015496193256, pvalue=9.259711640455711e-10)\n",
            "14 SpearmanrResult(correlation=0.36456152583947316, pvalue=5.836034240528533e-40)\n",
            "15 SpearmanrResult(correlation=0.4373430357619704, pvalue=1.2784517774799986e-58)\n",
            "16 SpearmanrResult(correlation=0.783921652715296, pvalue=1.8146494656772814e-256)\n",
            "17 SpearmanrResult(correlation=0.36015932611342893, pvalue=5.631962892784149e-39)\n",
            "18 SpearmanrResult(correlation=0.6555467438928378, pvalue=5.864500907284102e-152)\n",
            "19 SpearmanrResult(correlation=0.0667186032517969, pvalue=0.01927648255831786)\n",
            "20 SpearmanrResult(correlation=0.5585974292849151, pvalue=7.527142413053089e-102)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "607it [05:01,  2.63it/s]\n",
            "154it [00:23,  7.00it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.35422                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.22356628161672681, pvalue=2.131842332937802e-15)\n",
            "1 SpearmanrResult(correlation=0.41487980269464736, pvalue=2.333109005036306e-52)\n",
            "2 SpearmanrResult(correlation=0.1496727885909048, pvalue=1.3375672128367776e-07)\n",
            "3 SpearmanrResult(correlation=0.18202227610358737, pvalue=1.267055173899519e-10)\n",
            "4 SpearmanrResult(correlation=0.30807308653249543, pvalue=1.8807270243976339e-28)\n",
            "5 SpearmanrResult(correlation=0.7498184877907815, pvalue=1.615940058072511e-222)\n",
            "6 SpearmanrResult(correlation=0.2979730593958899, pvalue=1.2186275457221916e-26)\n",
            "7 SpearmanrResult(correlation=0.6580308794397478, pvalue=1.7233038294805462e-153)\n",
            "8 SpearmanrResult(correlation=0.20395370842180713, pvalue=5.134236739028933e-13)\n",
            "\u001b[93m Epoch 3/4 \t Train loss Q=3.0194 \t Val loss Q=3.2475 \t  Score Q=0.43877 \t Train loss A=2.5355 \t Val loss A=2.9105 \t Score A=0.35422  \t score=0.413406 \t time=651.19s \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "607it [05:00,  2.64it/s]\n",
            "154it [00:23,  6.99it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.43703                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.3750198924990147, pvalue=2.3153923763803446e-42)\n",
            "1 SpearmanrResult(correlation=0.6464447833798119, pvalue=1.8152833335796253e-146)\n",
            "2 SpearmanrResult(correlation=0.43543580629077633, pvalue=4.5379009005530036e-58)\n",
            "3 SpearmanrResult(correlation=0.27417910711530913, pvalue=1.1906493976408187e-22)\n",
            "4 SpearmanrResult(correlation=0.41150163185617883, pvalue=1.8583491445474155e-51)\n",
            "5 SpearmanrResult(correlation=0.40262628521005983, pvalue=3.87281488001904e-49)\n",
            "6 SpearmanrResult(correlation=0.35546085109592274, pvalue=6.088755383813907e-38)\n",
            "7 SpearmanrResult(correlation=0.5222664888611682, pvalue=5.101870880810066e-87)\n",
            "8 SpearmanrResult(correlation=0.6241677642875765, pvalue=8.611858223058136e-134)\n",
            "9 SpearmanrResult(correlation=0.1261107206749071, pvalue=9.160389402835418e-06)\n",
            "10 SpearmanrResult(correlation=0.510173490424803, pvalue=1.807096685625768e-82)\n",
            "11 SpearmanrResult(correlation=0.748195893001411, pvalue=4.851137166175303e-221)\n",
            "12 SpearmanrResult(correlation=0.3550897665380554, pvalue=7.335735093348942e-38)\n",
            "13 SpearmanrResult(correlation=0.1750145746503043, pvalue=6.430064906789732e-10)\n",
            "14 SpearmanrResult(correlation=0.3649038360531903, pvalue=4.885544632888513e-40)\n",
            "15 SpearmanrResult(correlation=0.4357794482518718, pvalue=3.613978107320396e-58)\n",
            "16 SpearmanrResult(correlation=0.7831303333670566, pvalue=1.305216581692673e-255)\n",
            "17 SpearmanrResult(correlation=0.3558427865926241, pvalue=5.024983760099446e-38)\n",
            "18 SpearmanrResult(correlation=0.6499156921256046, pvalue=1.5396925816588561e-148)\n",
            "19 SpearmanrResult(correlation=0.06620764893017837, pvalue=0.02022327839875646)\n",
            "20 SpearmanrResult(correlation=0.5602010192544316, pvalue=1.5104011081244412e-102)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "607it [05:00,  2.65it/s]\n",
            "154it [00:23,  6.97it/s]\n",
            "100%|██████████| 4/4 [43:22<00:00, 650.53s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r val_spearman-rho: 0.35735                                                                                                    \n",
            "0 SpearmanrResult(correlation=0.23347269115640948, pvalue=1.0879192306793804e-16)\n",
            "1 SpearmanrResult(correlation=0.41536875205410057, pvalue=1.7244104283921466e-52)\n",
            "2 SpearmanrResult(correlation=0.15843051341282688, pvalue=2.323073817158609e-08)\n",
            "3 SpearmanrResult(correlation=0.19351444652310365, pvalue=7.662975585038703e-12)\n",
            "4 SpearmanrResult(correlation=0.31496091676722787, pvalue=9.94393242935773e-30)\n",
            "5 SpearmanrResult(correlation=0.7477285865163545, pvalue=1.2859885936835154e-220)\n",
            "6 SpearmanrResult(correlation=0.29169658342620214, pvalue=1.4990991156791277e-25)\n",
            "7 SpearmanrResult(correlation=0.6505520963283843, pvalue=6.377633683389144e-149)\n",
            "8 SpearmanrResult(correlation=0.21041974993154985, pvalue=8.930406383704154e-14)\n",
            "\u001b[93m Epoch 4/4 \t Train loss Q=2.9199 \t Val loss Q=3.2614 \t  Score Q=0.43703 \t Train loss A=2.4033 \t Val loss A=2.9627 \t Score A=0.35735  \t score=0.413127 \t time=650.04s \u001b[0m\n",
            "Best Rhos: 0.41343949934301394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lAmKzPNX89t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4665b304-1488-4841-ffc9-be60c10df9bf"
      },
      "source": [
        "#The Rho's are not exactly right because I reran from fold>2 (check the log)\n",
        "np.mean(all_rhos) #Best Rhos: 0.4161831015275145"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40814141312915486"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfEnoVr5FB8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86c8d4d5-9e59-4621-da85-6ba8f39f3ec4"
      },
      "source": [
        "#THIS IS THE BEST RHO\n",
        "((0.43644 *21+ 0.36162*9)/30  + (0.43887*21 + 0.37817*9)/30  + (0.43403*21 + 0.39442*9)/30  + (0.43123*21 + 0.36439*9)/30 + (0.43901*21 + 0.36131*9)/30 ) /5 "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4167358"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foWlNvouX89x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3c042fc2-4476-4075-e926-a62bea8375e2"
      },
      "source": [
        "all_rhos"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.39624068987107164,\n",
              " 0.4091913761787977,\n",
              " 0.4079087894316589,\n",
              " 0.40814334031265537,\n",
              " 0.4014168467334519,\n",
              " 0.41569720881375893,\n",
              " 0.4134059818059639,\n",
              " 0.4131270718858805]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90QyZKfFD-3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccbc60bd-b8e1-4ad8-fcff-5ba994b0214a"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4167358"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eD4cyluX89z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Log for first 3 folds\n",
        "\n",
        "# WARNING:root:Epoch 1/4 \t Train loss Q=3.4740 \t Val loss Q=3.2735 \t  Score Q=0.42140 \t Train loss A=2.9563 \t Val loss A=2.8575 \t Score A=0.34180  \t score=0.397518 \t time=632.98s\n",
        "# WARNING:root:Epoch 2/4 \t Train loss Q=3.1800 \t Val loss Q=3.2323 \t  Score Q=0.43644 \t Train loss A=2.7395 \t Val loss A=2.8126 \t Score A=0.36162  \t score=0.413996 \t time=633.03s\n",
        "# WARNING:root:Epoch 3/4 \t Train loss Q=3.0293 \t Val loss Q=3.2350 \t  Score Q=0.43579 \t Train loss A=2.5497 \t Val loss A=2.9032 \t Score A=0.35433  \t score=0.411354 \t time=632.00s\n",
        "# WARNING:root:Epoch 4/4 \t Train loss Q=2.9316 \t Val loss Q=3.2509 \t  Score Q=0.43440 \t Train loss A=2.4209 \t Val loss A=2.9351 \t Score A=0.34896  \t score=0.408764 \t time=632.60s\n",
        "\n",
        "# WARNING:root:Epoch 1/4 \t Train loss Q=3.4762 \t Val loss Q=3.3138 \t  Score Q=0.41558 \t Train loss A=2.9663 \t Val loss A=2.8026 \t Score A=0.36065  \t score=0.399100 \t time=633.43s\n",
        "# WARNING:root:Epoch 2/4 \t Train loss Q=3.1721 \t Val loss Q=3.2465 \t  Score Q=0.43665 \t Train loss A=2.7491 \t Val loss A=2.7607 \t Score A=0.37817  \t score=0.419110 \t time=633.50s\n",
        "# WARNING:root:Epoch 3/4 \t Train loss Q=3.0190 \t Val loss Q=3.2451 \t  Score Q=0.43887 \t Train loss A=2.5622 \t Val loss A=2.8191 \t Score A=0.37341  \t score=0.419230 \t time=633.16s\n",
        "# WARNING:root:Epoch 4/4 \t Train loss Q=2.9192 \t Val loss Q=3.2632 \t  Score Q=0.43659 \t Train loss A=2.4302 \t Val loss A=2.8614 \t Score A=0.36607  \t score=0.415435 \t time=633.45s\n",
        "\n",
        "# WARNING:root:Epoch 1/4 \t Train loss Q=3.4953 \t Val loss Q=3.2774 \t  Score Q=0.41193 \t Train loss A=2.9427 \t Val loss A=2.8652 \t Score A=0.38094  \t score=0.402633 \t time=632.93s\n",
        "# WARNING:root:Epoch 2/4 \t Train loss Q=3.1822 \t Val loss Q=3.2347 \t  Score Q=0.42978 \t Train loss A=2.7365 \t Val loss A=2.8441 \t Score A=0.39442  \t score=0.419169 \t time=634.40s\n",
        "# WARNING:root:Epoch 3/4 \t Train loss Q=3.0290 \t Val loss Q=3.2249 \t  Score Q=0.43403 \t Train loss A=2.5501 \t Val loss A=2.8867 \t Score A=0.38308  \t score=0.418748 \t time=632.01s\n",
        "# WARNING:root:Epoch 4/4 \t Train loss Q=2.9303 \t Val loss Q=3.2448 \t  Score Q=0.43117 \t Train loss A=2.4134 \t Val loss A=2.9508 \t Score A=0.37948  \t score=0.415664 \t time=632.38s\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}