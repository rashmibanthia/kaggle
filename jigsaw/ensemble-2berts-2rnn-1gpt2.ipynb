{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Berts + GPT2 + 2RNN (In the ratio : 28 bert + 27 bert + 16 rnn + 19 gpt2 + 11 rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import fastai\n",
    "from fastai.train import Learner\n",
    "from fastai.train import DataBunch\n",
    "from fastai.callbacks import *\n",
    "from fastai.basic_data import DatasetType\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import collections\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "#from tqdm import tqdm, tqdm_notebook\n",
    "#import fastprogress\n",
    "#from fastprogress import force_console_behavior\n",
    "#from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "#tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = '../input/jigsawdatasets/pytorch-pretrained-gpt2/pytorch-pretrained-gpt2/'\n",
    "#sys.path.append(package_dir)\n",
    "sys.path.insert(1,package_dir)\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "from pytorch_pretrained_bert import GPT2Tokenizer, GPT2ClassificationHeadModel\n",
    "from pytorch_pretrained_bert import OpenAIAdam\n",
    "from pytorch_pretrained_bert import GPT2Config\n",
    "\n",
    "# # disable progress bars when submitting\n",
    "# def is_interactive():\n",
    "#    return 'SHLVL' not in os.environ\n",
    "\n",
    "# if not is_interactive():\n",
    "#     def nop(it, *a, **k):\n",
    "#         return it\n",
    "#     tqdm = nop\n",
    "#     fastprogress.fastprogress.NO_BAR = True\n",
    "#     master_bar, progress_bar = force_console_behavior()\n",
    "#     fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package_dir = '../input/jigsawdatasets/pytorch-pretrained-gpt2/pytorch_pretrained_gpt2/pytorch_pretrained_bert'\n",
    "# sys.path.insert(1,package_dir)\n",
    "\n",
    "# from pytorch_pretrained_bert  import GPT2Tokenizer, GPT2ClassificationHeadModel\n",
    "# # from pytorch_pretrained_bert import OpenAIAdam\n",
    "# # from pytorch_pretrained_bert import GPT2Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "device = torch.device('cuda')\n",
    "\n",
    "    \n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"Trump's\" : 'trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n",
    "    'ain;t': 'am not','ainÂ´t': 'am not','ainâ€™t': 'am not',\"aren't\": 'are not',\n",
    "    'aren,t': 'are not','aren;t': 'are not','arenÂ´t': 'are not','arenâ€™t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n",
    "    'can;t': 'cannot','can;t;ve': 'cannot have',\n",
    "    'canÂ´t': 'cannot','canÂ´tÂ´ve': 'cannot have','canâ€™t': 'cannot','canâ€™tâ€™ve': 'cannot have',\n",
    "    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n",
    "    'couldn;t;ve': 'could not have','couldnÂ´t': 'could not',\n",
    "    'couldnÂ´tÂ´ve': 'could not have','couldnâ€™t': 'could not','couldnâ€™tâ€™ve': 'could not have','couldÂ´ve': 'could have',\n",
    "    'couldâ€™ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didnÂ´t': 'did not',\n",
    "    'didnâ€™t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesnÂ´t': 'does not',\n",
    "    'doesnâ€™t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','donÂ´t': 'do not','donâ€™t': 'do not',\n",
    "    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n",
    "    'hadn;t;ve': 'had not have','hadnÂ´t': 'had not','hadnÂ´tÂ´ve': 'had not have','hadnâ€™t': 'had not','hadnâ€™tâ€™ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasnÂ´t': 'has not','hasnâ€™t': 'has not',\n",
    "    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','havenÂ´t': 'have not','havenâ€™t': 'have not',\"he'd\": 'he would',\n",
    "    \"he'd've\": 'he would have',\"he'll\": 'he will',\n",
    "    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n",
    "    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','heÂ´d': 'he would','heÂ´dÂ´ve': 'he would have','heÂ´ll': 'he will',\n",
    "    'heÂ´s': 'he is','heâ€™d': 'he would','heâ€™dâ€™ve': 'he would have','heâ€™ll': 'he will','heâ€™s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n",
    "    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n",
    "    'how;s': 'how is','howÂ´d': 'how did','howÂ´ll': 'how will','howÂ´s': 'how is','howâ€™d': 'how did','howâ€™ll': 'how will',\n",
    "    'howâ€™s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n",
    "    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n",
    "    'isn,t': 'is not','isn;t': 'is not','isnÂ´t': 'is not','isnâ€™t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n",
    "    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','itÂ´d': 'it would','itÂ´ll': 'it will','itÂ´s': 'it is',\n",
    "    'itâ€™d': 'it would','itâ€™ll': 'it will','itâ€™s': 'it is',\n",
    "    'iÂ´d': 'i would','iÂ´ll': 'i will','iÂ´m': 'i am','iÂ´ve': 'i have','iâ€™d': 'i would','iâ€™ll': 'i will','iâ€™m': 'i am',\n",
    "    'iâ€™ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','letÂ´s': 'let us',\n",
    "    'letâ€™s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n",
    "    'maynÂ´t': 'may not','maynâ€™t': 'may not','maÂ´am': 'madam','maâ€™am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightnÂ´t': 'might not',\n",
    "    'mightnâ€™t': 'might not','mightÂ´ve': 'might have','mightâ€™ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n",
    "    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustnÂ´t': 'must not','mustnâ€™t': 'must not','mustÂ´ve': 'must have',\n",
    "    'mustâ€™ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','neednÂ´t': 'need not','neednâ€™t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n",
    "    'oughtnÂ´t': 'ought not','oughtnâ€™t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n",
    "    'shan,t': 'shall not','shan;t': 'shall not','shanÂ´t': 'shall not','shanâ€™t': 'shall not','shaÂ´nÂ´t': 'shall not','shaâ€™nâ€™t': 'shall not',\n",
    "    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n",
    "    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','sheÂ´d': 'she would','sheÂ´ll': 'she will',\n",
    "    'sheÂ´s': 'she is','sheâ€™d': 'she would','sheâ€™ll': 'she will','sheâ€™s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n",
    "    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldnÂ´t': 'should not','shouldnâ€™t': 'should not','shouldÂ´ve': 'should have',\n",
    "    'shouldâ€™ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n",
    "    'that;s': 'that is','thatÂ´d': 'that would','thatÂ´s': 'that is','thatâ€™d': 'that would','thatâ€™s': 'that is',\"there'd\": 'there had',\n",
    "    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n",
    "    'thereÂ´d': 'there had','thereÂ´s': 'there is','thereâ€™d': 'there had','thereâ€™s': 'there is',\n",
    "    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n",
    "    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n",
    "    'they;ve': 'they have','theyÂ´d': 'they would','theyÂ´ll': 'they will','theyÂ´re': 'they are','theyÂ´ve': 'they have','theyâ€™d': 'they would','theyâ€™ll': 'they will',\n",
    "    'theyâ€™re': 'they are','theyâ€™ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasnÂ´t': 'was not',\n",
    "    'wasnâ€™t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n",
    "    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n",
    "    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','werenÂ´t': 'were not','werenâ€™t': 'were not','weÂ´d': 'we would','weÂ´ll': 'we will',\n",
    "    'weÂ´re': 'we are','weÂ´ve': 'we have','weâ€™d': 'we would','weâ€™ll': 'we will','weâ€™re': 'we are','weâ€™ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n",
    "    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n",
    "    'what;s': 'what is','what;ve': 'what have','whatÂ´ll': 'what will',\n",
    "    'whatÂ´re': 'what are','whatÂ´s': 'what is','whatÂ´ve': 'what have','whatâ€™ll': 'what will','whatâ€™re': 'what are','whatâ€™s': 'what is',\n",
    "    'whatâ€™ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n",
    "    'where;s': 'where is','whereÂ´d': 'where did','whereÂ´s': 'where is','whereâ€™d': 'where did','whereâ€™s': 'where is',\n",
    "    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n",
    "    'whoÂ´ll': 'who will','whoÂ´s': 'who is','whoâ€™ll': 'who will','whoâ€™s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n",
    "    'wonÂ´t': 'will not','wonâ€™t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldnÂ´t': 'would not',\n",
    "    'wouldnâ€™t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n",
    "    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n",
    "    'you;re': 'you are','youÂ´d': 'you would','youÂ´ll': 'you will','youÂ´re': 'you are','youâ€™d': 'you would','youâ€™ll': 'you will','youâ€™re': 'you are',\n",
    "    'Â´cause': 'because','â€™cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n",
    "    \"havn't\": 'have not',\"hereâ€™s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n",
    "    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n",
    "    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n",
    "    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"youâ€™ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n",
    "    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"â€˜I\":'I',\n",
    "    'á´€É´á´…':'and','á´›Êœá´‡':'the','Êœá´á´á´‡':'home','á´œá´˜':'up','Ê™Ê':'by','á´€á´›':'at','â€¦and':'and','civilbeat':'civil beat',\\\n",
    "    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','á´„Êœá´‡á´„á´‹':'check','Ò“á´Ê€':'for','á´›ÊœÉªs':'this','á´„á´á´á´˜á´œá´›á´‡Ê€':'computer',\\\n",
    "    'á´á´É´á´›Êœ':'month','á´¡á´Ê€á´‹ÉªÉ´É¢':'working','á´Šá´Ê™':'job','Ò“Ê€á´á´':'from','Sá´›á´€Ê€á´›':'start','gubmit':'submit','COâ‚‚':'carbon dioxide','Ò“ÉªÊ€sá´›':'first',\\\n",
    "    'á´‡É´á´…':'end','á´„á´€É´':'can','Êœá´€á´ á´‡':'have','á´›á´':'to','ÊŸÉªÉ´á´‹':'link','á´Ò“':'of','Êœá´á´œÊ€ÊŸÊ':'hourly','á´¡á´‡á´‡á´‹':'week','á´‡É´á´…':'end','á´‡xá´›Ê€á´€':'extra',\\\n",
    "    'GÊ€á´‡á´€á´›':'great','sá´›á´œá´…á´‡É´á´›s':'student','sá´›á´€Ê':'stay','á´á´á´s':'mother','á´Ê€':'or','á´€É´Êá´É´á´‡':'anyone','É´á´‡á´‡á´…ÉªÉ´É¢':'needing','á´€É´':'an','ÉªÉ´á´„á´á´á´‡':'income',\\\n",
    "    'Ê€á´‡ÊŸÉªá´€Ê™ÊŸá´‡':'reliable','Ò“ÉªÊ€sá´›':'first','Êá´á´œÊ€':'your','sÉªÉ¢É´ÉªÉ´É¢':'signing','Ê™á´á´›á´›á´á´':'bottom','Ò“á´ÊŸÊŸá´á´¡ÉªÉ´É¢':'following','Má´€á´‹á´‡':'make',\\\n",
    "    'á´„á´É´É´á´‡á´„á´›Éªá´É´':'connection','ÉªÉ´á´›á´‡Ê€É´á´‡á´›':'internet','financialpost':'financial post', 'Êœaá´ á´‡':' have ', 'á´„aÉ´':' can ', 'Maá´‹á´‡':' make ', 'Ê€á´‡ÊŸÉªaÊ™ÊŸá´‡':' reliable ', 'É´á´‡á´‡á´…':' need ',\n",
    "    'á´É´ÊŸÊ':' only ', 'á´‡xá´›Ê€a':' extra ', 'aÉ´':' an ', 'aÉ´Êá´É´á´‡':' anyone ', 'sá´›aÊ':' stay ', 'Sá´›aÊ€á´›':' start', 'SHOPO':'shop',\n",
    "    }\n",
    "mispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','GÊ€á´‡at':'great','Ê™á´á´›toá´':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yá´á´œ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','á´€':'a', 'ðŸ˜‰':'wink','ðŸ˜‚':'joy','ðŸ˜€':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\n",
    "\n",
    "special_punc_mappings = {\"â€”\": \"-\", \"â€“\": \"-\", \"_\": \"-\", 'â€': '\"', \"â€³\": '\"', 'â€œ': '\"', 'â€¢': '.', 'âˆ’': '-',\n",
    "                         \"â€™\": \"'\", \"â€˜\": \"'\", \"Â´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','ØŒ':'','â€ž':'',\n",
    "                         'â€¦': ' ... ', '\\ufeff': ''}\n",
    "\n",
    "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "\n",
    "rare_words_mapping = {' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA','u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA', ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',\n",
    "                      ' U.s ': 'USA', ' u.S ': ' USA ', 'fu.k': 'fuck', 'U.K.': 'UK', ' u.k ': ' UK ',' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n",
    "                      'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n",
    "                      'Ra apist': 'Rapist', '2fifth': 'twenty fifth', '2third': 'twenty third','2nineth': 'twenty nineth', '2fourth': 'twenty fourth', '#metoo': 'MeToo',\n",
    "                      'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n",
    "                      'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin','fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n",
    "                      'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n",
    "                      'culturr': 'culture','weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n",
    "                      'indans': 'indians', 'mastuburate': 'masturbate', 'f**k': 'fuck', 'F**k': 'fuck', 'F**K': 'fuck',\n",
    "                      ' u r ': ' you are ', ' u ': ' you ', 'æ“ä½ å¦ˆ': 'fuck your mother', 'e.g.': 'for example',\n",
    "                      'i.e.': 'in other words', '...': '.', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n",
    "                      'f***': 'fuck', 'f**': 'fuc', 'F***': 'fuck', 'F**': 'fuc','a****': 'assho', 'a**': 'ass', 'h***': 'hole', 'A****': 'assho', 'A**': 'ass', 'H***': 'hole',\n",
    "                      's***': 'shit', 's**': 'shi', 'S***': 'shit', 'S**': 'shi', 'Sh**': 'shit',\n",
    "                      'p****': 'pussy', 'p*ssy': 'pussy', 'P****': 'pussy','p***': 'porn', 'p*rn': 'porn', 'P***': 'porn',\n",
    "                      'st*up*id': 'stupid','d***': 'dick', 'di**': 'dick', 'h*ck': 'hack',\n",
    "                      'b*tch': 'bitch', 'bi*ch': 'bitch', 'bit*h': 'bitch', 'bitc*': 'bitch', 'b****': 'bitch',\n",
    "                      'b***': 'bitc', 'b**': 'bit', 'b*ll': 'bull'\n",
    "                      }\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', 'â€¢',  '~', '@', 'Â£',\n",
    "    'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',\n",
    "    'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', 'Â½', 'Ã ', 'â€¦', 'â€œ', 'â˜…', 'â€',\n",
    "    'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾',\n",
    "    'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€', 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼',\n",
    "    'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²',\n",
    "    'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆž', 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»',\n",
    "    'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜',\n",
    "    'Â¹', 'â‰¤', 'â€¡', 'âˆš', 'Â«', 'Â»', 'Â´', 'Âº', 'Â¾', 'Â¡', 'Â§', 'Â£', 'â‚¤']\n",
    "\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "\n",
    "def pre_clean_rare_words(text):\n",
    "    for rare_word in rare_words_mapping:\n",
    "        if rare_word in text:\n",
    "            text = text.replace(rare_word, rare_words_mapping[rare_word])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LenMatchBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __iter__(self):\n",
    "        buckets = [[]] * 100\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            count_zeros = torch.sum(self.sampler.data_source[idx][0] == 0)\n",
    "            count_zeros = int(count_zeros / 64) \n",
    "            if len(buckets[count_zeros]) == 0:  buckets[count_zeros] = []\n",
    "\n",
    "            buckets[count_zeros].append(idx)\n",
    "\n",
    "            if len(buckets[count_zeros]) == self.batch_size:\n",
    "                batch = list(buckets[count_zeros])\n",
    "                #print(\"batch:\",batch)\n",
    "                yield batch\n",
    "                yielded += 1\n",
    "                buckets[count_zeros] = []\n",
    "\n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yielded += 1\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yielded += 1\n",
    "            yield batch\n",
    "\n",
    "        assert len(self) == yielded, \"produced an inccorect number of batches. expected %i, but yielded %i\" %(len(self), yielded)\n",
    "\n",
    "def trim_tensors(tsrs):\n",
    "    max_len = torch.max(torch.sum( (tsrs[0] != 0  ), 1))\n",
    "    tsrs1 = []\n",
    "    if max_len > 2:         \n",
    "        tsrs = [tsr[:, :max_len] if i!=1 else tsr for i,tsr in enumerate(tsrs) ]\n",
    "        \n",
    "    return tsrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_base(paths, train, test):\n",
    "    \n",
    "    NUM_MODELS = len(paths)\n",
    "    def convert_lines(example, max_seq_length,tokenizer):\n",
    "        max_seq_length -=2\n",
    "        all_tokens = []\n",
    "        longer = 0\n",
    "        #for text in tqdm(example):\n",
    "        for text in example:\n",
    "            tokens_a = tokenizer.tokenize(text)\n",
    "            if len(tokens_a)>max_seq_length:\n",
    "                tokens_a = tokens_a[:max_seq_length]\n",
    "                longer += 1\n",
    "            one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "            all_tokens.append(one_token)\n",
    "        return np.array(all_tokens)\n",
    "    \n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH = 220 + 2\n",
    "    SEED = 1234\n",
    "    BATCH_SIZE = 32\n",
    "    BERT_MODEL_PATH = '../input/jigsawdatasets/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    #All I need from bert_inference  is bert_config.json\n",
    "    #bert_config = BertConfig('../input/jigsawdatasets/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_config.json')\n",
    "    \n",
    "    bert_config = BertConfig(vocab_size_or_config_json_file=30522, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072,hidden_act=\"gelu\",\n",
    "                        hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512, type_vocab_size=2,initializer_range=0.02 ) \n",
    "#                 layer_norm_eps=1e-12)\n",
    "                \n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "\n",
    "    test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n",
    "    test_df['id1'] = np.arange(len(test_df))\n",
    "    test_df['comment_text'] = test_df['comment_text'].astype(str) \n",
    "    test_df['comment_text'] = test_df['comment_text'].apply(lambda x: correct_contraction(x, contraction_mapping))\n",
    "    X_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "    \n",
    "    bert_base_preds = []\n",
    "    for k in range(NUM_MODELS):\n",
    "        model = BertForSequenceClassification(bert_config, num_labels=8)\n",
    "        #model.load_state_dict(torch.load(\"../input/bertmodel-jigsaw/bert_pytorch_mymodel.bin\"))\n",
    "        model.load_state_dict(torch.load(paths[k]))\n",
    "\n",
    "        model.to(device)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.eval()\n",
    "\n",
    "        test_preds = np.zeros((len(X_test)))\n",
    "        \n",
    "        test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long),\n",
    "                                              torch.tensor(test_df['id1'],dtype=torch.long),\n",
    "                                              torch.from_numpy(np.array(X_test>0, dtype=np.uint8)),\n",
    "                                              torch.zeros(X_test.shape),  torch.zeros(X_test.shape))\n",
    "\n",
    "        ran_sampler = torch.utils.data.RandomSampler(test_dataset)\n",
    "        len_sampler = LenMatchBatchSampler(ran_sampler, batch_size=32, drop_last=False)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset,batch_sampler=len_sampler)\n",
    "        \n",
    "#         test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n",
    "#         test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "        #tk0 = tqdm(test_loader)\n",
    "        tk0 = test_loader\n",
    "        for i, x_batch in enumerate(tk0):\n",
    "            tsrs = trim_tensors(x_batch)\n",
    "            #pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n",
    "            #test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "            b_input_ids, ids, b_input_mask, b_segment_ids,b_label = tuple(t.to(device) for t in tsrs)\n",
    "            pred = model(b_input_ids,attention_mask=b_input_mask, labels=None)    \n",
    "            test_preds[ids.detach().cpu().numpy()] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "\n",
    "\n",
    "        test_pred_base = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n",
    "\n",
    "        bert_base_preds.append(test_pred_base)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return bert_base_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bert_large(large_model_dir_base, large_model_dir_path,test):\n",
    "#     # This function is not used by this file but is still used by the Colab and\n",
    "#     # people who depend on it.\n",
    "#     def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "#                                      tokenizer):\n",
    "#         \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "#         features = []\n",
    "#         for (ex_index, example) in enumerate(examples):\n",
    "#             if ex_index % 10000 == 0:\n",
    "#                 tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "#             feature = convert_single_example(ex_index, example, label_list,\n",
    "#                                      max_seq_length, tokenizer)\n",
    "            \n",
    "#             features.append(feature)            \n",
    "#         return features\n",
    "    \n",
    "    \n",
    "\n",
    "#     def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "#                                tokenizer):\n",
    "#         \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "\n",
    "#         if isinstance(example, PaddingInputExample):\n",
    "#             return InputFeatures(\n",
    "#                 input_ids=[0] * max_seq_length,\n",
    "#                 input_mask=[0] * max_seq_length,\n",
    "#                 segment_ids=[0] * max_seq_length,\n",
    "#                 label_id=[0]*len(LABEL_COLUMNS),\n",
    "#                 is_real_example=False)\n",
    "\n",
    "#         label_map = {}\n",
    "#         for (i, label) in enumerate(label_list):\n",
    "#             label_map[label] = i\n",
    "\n",
    "#         tokens_a = tokenizer.tokenize(example.text_a)\n",
    "#         tokens_b = None\n",
    "#         if example.text_b:\n",
    "#             tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "#         if tokens_b:\n",
    "#             # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "#             # length is less than the specified length.\n",
    "#             # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "#             _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "#         else:\n",
    "#             # Account for [CLS] and [SEP] with \"- 2\"\n",
    "#             if len(tokens_a) > max_seq_length - 2:\n",
    "#                 tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "#         # The convention in BERT is:\n",
    "#         # (a) For sequence pairs:\n",
    "#         #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "#         #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "#         # (b) For single sequences:\n",
    "#         #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "#         #  type_ids: 0     0   0   0  0     0 0\n",
    "#         #\n",
    "#         # Where \"type_ids\" are used to indicate whether this is the first\n",
    "#         # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "#         # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "#         # embedding vector (and position vector). This is not *strictly* necessary\n",
    "#         # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "#         # it easier for the model to learn the concept of sequences.\n",
    "#         #\n",
    "#         # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "#         # used as the \"sentence vector\". Note that this only makes sense because\n",
    "#         # the entire model is fine-tuned.\n",
    "#         tokens = []\n",
    "#         segment_ids = []\n",
    "#         tokens.append(\"[CLS]\")\n",
    "#         segment_ids.append(0)\n",
    "#         for token in tokens_a:\n",
    "#             tokens.append(token)\n",
    "#             segment_ids.append(0)\n",
    "#         tokens.append(\"[SEP]\")\n",
    "#         segment_ids.append(0)\n",
    "\n",
    "#         if tokens_b:\n",
    "#             for token in tokens_b:\n",
    "#                 tokens.append(token)\n",
    "#                 segment_ids.append(1)\n",
    "#             tokens.append(\"[SEP]\")\n",
    "#             segment_ids.append(1)\n",
    "\n",
    "#         input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "#         # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "#         # tokens are attended to.\n",
    "#         input_mask = [1] * len(input_ids)\n",
    "\n",
    "#         # Zero-pad up to the sequence length.\n",
    "#         while len(input_ids) < max_seq_length:\n",
    "#             input_ids.append(0)\n",
    "#             input_mask.append(0)\n",
    "#             segment_ids.append(0)\n",
    "\n",
    "#         assert len(input_ids) == max_seq_length\n",
    "#         assert len(input_mask) == max_seq_length\n",
    "#         assert len(segment_ids) == max_seq_length\n",
    "\n",
    "#         #label_id = label_map[example.label]\n",
    "#         labels_ids = []\n",
    "#         #print(ex_index, example.label)\n",
    "#         for label in example.label:\n",
    "#             labels_ids.append(float(label))\n",
    "\n",
    "#         if ex_index < 5:\n",
    "#             tf.logging.info(\"*** Example ***\")\n",
    "#             tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "#             tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "#                 [tokenization.printable_text(x) for x in tokens]))\n",
    "#             tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#             tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#             tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "#             tf.logging.info(\"label: %s (id = %s)\" % (example.label, labels_ids))\n",
    "\n",
    "#         feature = InputFeatures(\n",
    "#                             input_ids=input_ids,\n",
    "#                             input_mask=input_mask,\n",
    "#                             segment_ids=segment_ids,\n",
    "#                             label_id=labels_ids,\n",
    "#                             is_real_example=True)\n",
    "#         return feature\n",
    "\n",
    "\n",
    "#     def file_based_convert_examples_to_features(\n",
    "#         examples, label_list, max_seq_length, tokenizer, output_file):\n",
    "#         \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
    "\n",
    "#         writer = tf.python_io.TFRecordWriter(output_file)\n",
    "#         def create_int_feature(values):\n",
    "#             f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "#             return f\n",
    "\n",
    "#         def create_float_feature(values):\n",
    "#             f = tf.train.Feature(float_list = tf.train.FloatList(value=list(values)))\n",
    "#             return f\n",
    "\n",
    "#         for (ex_index, example) in enumerate(examples):\n",
    "#             if ex_index % 10000 == 0:\n",
    "#                 tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "#             feature = convert_single_example(ex_index, example, label_list,\n",
    "#                                          max_seq_length, tokenizer)\n",
    "\n",
    "# #             def create_int_feature(values):\n",
    "# #                 f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "# #                 return f\n",
    "\n",
    "# #             def create_float_feature(values):\n",
    "# #                 f = tf.train.Feature(float_list = tf.train.FloatList(value=list(values)))\n",
    "# #                 return f\n",
    "\n",
    "#             features = collections.OrderedDict()\n",
    "#             features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "#             features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "#             features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "\n",
    "#             features[\"label_ids\"] = create_float_feature(feature.label_id)\n",
    "\n",
    "#             features[\"is_real_example\"] = create_int_feature(\n",
    "#                 [int(feature.is_real_example)])\n",
    "\n",
    "#             tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "#             writer.write(tf_example.SerializeToString())\n",
    "            \n",
    "#         writer.close()\n",
    "    \n",
    "#     #del test  \n",
    "#     gc.collect()\n",
    "    \n",
    "#     export_dir_base = '../input/tf-large-bert-inference-2/1560187254-20190610t173847z-001'\n",
    "#     export_dir_base = large_model_dir_base\n",
    "#     data_dir = '/kaggle/working'\n",
    "#     vocab_file = '../input/jigsawdatasets/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "\n",
    "#     from tensorflow.contrib import predictor\n",
    "\n",
    "#     predict_fn_e = predictor.from_saved_model(export_dir_base + large_model_dir_path)\n",
    "\n",
    "\n",
    "#     #test=pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "#     test['comment_text'] = test['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True).replace(r'\\t',  ' ', regex=True)\n",
    "\n",
    "#     test.to_csv( 'test.tsv', sep='\\t', index=False, header=True)\n",
    "\n",
    "#     processor = ColaProcessor()\n",
    "#     #label_list = processor.get_labels()\n",
    "#     label_list = [0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "\n",
    "\n",
    "#     predict_examples = processor.get_test_examples(data_dir)\n",
    "\n",
    "#     tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
    "\n",
    "\n",
    "#     max_seq_length = 222\n",
    "#     #num_actual_predict_examples = len(predict_examples)\n",
    "#     #predict_examples = predict_examples + predict_examples[0:24]\n",
    "\n",
    "#     i_features = convert_examples_to_features(predict_examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "\n",
    "#     k = len(i_features)\n",
    "#     input_ids_list = []\n",
    "#     input_mask_list = []\n",
    "#     label_ids_list = []\n",
    "#     segment_ids_list = []\n",
    "#     for i in range(k):\n",
    "#        input_ids_list.append(i_features[i].input_ids)\n",
    "#        input_mask_list.append(i_features[i].input_mask)\n",
    "#        #label_ids_list.append(i_features[i].label_id)\n",
    "#        segment_ids_list.append(i_features[i].segment_ids) \n",
    "\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     #prediction with batches \n",
    "\n",
    "# #    start = time.time()\n",
    "# #     print(\"--------------------------------------------------------\")\n",
    "# #     print(\"Starting infer ...\")\n",
    "# #     print(\"--------------------------------------------------------\")\n",
    "#     batch_size = 512 #1280 #1024\n",
    "\n",
    "\n",
    "\n",
    "#     k = (int)(np.ceil(len(predict_examples)/batch_size))\n",
    "#     #tq = tqdm_notebook(range(k))\n",
    "#     tq =  range(k) \n",
    "#     preds = np.array(np.zeros((len(predict_examples),8)))\n",
    "#     for i in tq:\n",
    "#     #    tmp = predict_fn_e({'input_ids': input_ids_list[i * batch_size:(i+1) * batch_size] , \n",
    "#     #                        'input_mask': input_mask_list[i * batch_size:(i+1) * batch_size], \n",
    "#     #                        #'label_ids': label_ids_list[i * batch_size:(i+1) * batch_size],\n",
    "#     #                        'segment_ids':segment_ids_list[i * batch_size:(i+1) * batch_size] } )['probabilities']\n",
    "#     #    print(tmp.shape)\n",
    "\n",
    "#         preds[i * batch_size:(i+1) * batch_size,:] = predict_fn_e({'input_ids': input_ids_list[i * batch_size:(i+1) * batch_size] , \n",
    "#                            'input_mask': input_mask_list[i * batch_size:(i+1) * batch_size], \n",
    "#                            #'label_ids': label_ids_list[i * batch_size:(i+1) * batch_size],\n",
    "#                            'segment_ids':segment_ids_list[i * batch_size:(i+1) * batch_size] } )['probabilities']\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     end = time.time()\n",
    "# #     print(\"--------------------------------------------------------\")\n",
    "# #     print(\"Infer complete in \", end - start, \" seconds\")\n",
    "# #     print(\"--------------------------------------------------------\")\n",
    "    \n",
    "#     del predict_fn_e,predict_examples, processor, input_ids_list, input_mask_list, segment_ids_list\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     tf.reset_default_graph()\n",
    "    \n",
    "#     return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_lstm(paths, train, test):\n",
    "    \n",
    "    NUM_MODELS = len(paths)\n",
    "    CRAWL_EMBEDDING_PATH = '../input/jigsawdatasets/crawl-300d-2M.pkl'\n",
    "    GLOVE_EMBEDDING_PATH = '../input/jigsawdatasets/glove.840B.300d.pkl'\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    def load_embeddings(path):\n",
    "        with open(path,'rb') as f:\n",
    "            emb_arr = pickle.load(f)\n",
    "        return emb_arr\n",
    "\n",
    "    def build_matrix(word_index, path):\n",
    "        embedding_index = load_embeddings(path)\n",
    "        embedding_matrix = np.zeros((max_features + 1, 300))\n",
    "        unknown_words = []\n",
    "\n",
    "        for word, i in word_index.items():\n",
    "            if i <= max_features:\n",
    "                try:\n",
    "                    embedding_matrix[i] = embedding_index[word]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        embedding_matrix[i] = embedding_index[word.lower()]\n",
    "                    except KeyError:\n",
    "                        try:\n",
    "                            embedding_matrix[i] = embedding_index[word.title()]\n",
    "                        except KeyError:\n",
    "                            unknown_words.append(word)\n",
    "        return embedding_matrix, unknown_words\n",
    "    \n",
    "    test['comment_text'] = test['comment_text'].astype(str).fillna(\"DUMMY_VALUE\")\n",
    "    \n",
    "    symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\n",
    "    symbols_to_delete = '\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\n",
    "\n",
    "    from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "    remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "    def handle_punctuation(x):\n",
    "        x = x.translate(remove_dict)\n",
    "        x = x.translate(isolate_dict)\n",
    "        return x\n",
    "\n",
    "    def handle_contractions(x):\n",
    "        x = tokenizer.tokenize(x)\n",
    "        return x\n",
    "    \n",
    "    def fix_quote(x):\n",
    "        x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "        x = ' '.join(x)\n",
    "        return x\n",
    "\n",
    "    def preprocess(x):\n",
    "        x = handle_punctuation(x)\n",
    "        x = handle_contractions(x)\n",
    "        x = fix_quote(x)\n",
    "\n",
    "        x = x.replace(\"n't\",'not')\n",
    "        x = x.replace(\"N'T\",'NOT')\n",
    "        x = x.replace('tRump','Trump')\n",
    "        x = x.replace(\"gov't\",\"government\")\n",
    "        x = x.replace(\"Gov't\",\"Government\")\n",
    "        x = x.replace(\"Twitler\",\"Twitter\")\n",
    "        return x\n",
    "    \n",
    "    #x_train = train['comment_text'].progress_apply(lambda x:preprocess(x))\n",
    "    x_train = train['comment_text'].apply(lambda x:preprocess(x))\n",
    "    y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "    x_test = test['comment_text'].apply(lambda x:preprocess(x))\n",
    "    #x_test = test['comment_text'].progress_apply(lambda x:preprocess(x))\n",
    "\n",
    "    TOXICITY_COLUMN = 'target'\n",
    "    identity_columns = ['asian', 'atheist',\n",
    "           'bisexual', 'black', 'buddhist', 'christian', 'female',\n",
    "           'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n",
    "           'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n",
    "           'muslim', 'other_disability', 'other_gender',\n",
    "           'other_race_or_ethnicity', 'other_religion',\n",
    "           'other_sexual_orientation', 'physical_disability',\n",
    "           'psychiatric_or_mental_illness', 'transgender', 'white']\n",
    "\n",
    "    subgroup_bool_train = train[identity_columns].fillna(0)>=0.5\n",
    "    toxic_bool_train = train[TOXICITY_COLUMN].fillna(0)>=0.5\n",
    "    subgroup_negative_mask = subgroup_bool_train.values.sum(axis=1).astype(bool) & ~toxic_bool_train\n",
    "    # Overall\n",
    "    weights = np.ones((len(train),))\n",
    "    # Subgroup negative\n",
    "    weights += subgroup_negative_mask\n",
    "    loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "    #y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n",
    "    \n",
    "    max_features = 400000\n",
    "    tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False, oov_token = 'UNK')\n",
    "    tokenizer.fit_on_texts(list(x_train)+list(x_test))\n",
    "    print(len(tokenizer.word_counts)) \n",
    "\n",
    "    crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "    print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "\n",
    "    glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "    print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "\n",
    "    max_features = max_features or len(tokenizer.word_index) + 1\n",
    "    embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "    \n",
    "    del crawl_matrix\n",
    "    del glove_matrix\n",
    "    gc.collect()\n",
    "\n",
    "    #x_train_torch = torch.tensor(x_train, dtype=torch.long)\n",
    "    #y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #x_train = tokenizer.texts_to_sequences(x_train)\n",
    "    x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "    #lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    "\n",
    "    maxlen = 300\n",
    "    #x_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))\n",
    "    #print(x_train_padded.shape)\n",
    "\n",
    "    # lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    "    test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n",
    "    x_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))\n",
    "    \n",
    "    del x_train, x_test, tokenizer\n",
    "    gc.collect()\n",
    "    \n",
    "    class SequenceBucketCollator():\n",
    "        def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "            self.choose_length = choose_length\n",
    "            self.sequence_index = sequence_index\n",
    "            self.length_index = length_index\n",
    "            self.label_index = label_index\n",
    "\n",
    "        def __call__(self, batch):\n",
    "            batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "\n",
    "            sequences = batch[self.sequence_index]\n",
    "            lengths = batch[self.length_index]\n",
    "\n",
    "            length = self.choose_length(lengths)\n",
    "            mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "            padded_sequences = sequences[:, mask]\n",
    "\n",
    "            batch[self.sequence_index] = padded_sequences\n",
    "\n",
    "            if self.label_index is not None:\n",
    "                return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "\n",
    "            return batch\n",
    "        \n",
    "    LSTM_UNITS = 128\n",
    "    DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    class SpatialDropout(nn.Dropout2d):\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "            x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "            x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "            x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "            x = x.squeeze(2)  # (N, T, K)\n",
    "            return x\n",
    "        \n",
    "    class NeuralNet(nn.Module):\n",
    "        def __init__(self, embedding_matrix, num_aux_targets):\n",
    "            super(NeuralNet, self).__init__()\n",
    "            embed_size = embedding_matrix.shape[1]\n",
    "\n",
    "            self.embedding = nn.Embedding(max_features, embed_size)\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            self.embedding_dropout = SpatialDropout(0.2)\n",
    "\n",
    "            self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "            self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "\n",
    "            self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "            self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "\n",
    "            self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "            self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "\n",
    "        def forward(self, x, lengths=None):\n",
    "            h_embedding = self.embedding(x.long())\n",
    "            h_embedding = self.embedding_dropout(h_embedding)\n",
    "\n",
    "            h_lstm1, _ = self.lstm1(h_embedding)\n",
    "            h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "\n",
    "            # global average pooling\n",
    "            avg_pool = torch.mean(h_lstm2, 1)\n",
    "            # global max pooling\n",
    "            max_pool, _ = torch.max(h_lstm2, 1)\n",
    "\n",
    "            h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "            h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "            h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "\n",
    "            hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "\n",
    "            result = self.linear_out(hidden)\n",
    "            aux_result = self.linear_aux_out(hidden)\n",
    "            out = torch.cat([result, aux_result], 1)\n",
    "\n",
    "            return out\n",
    "\n",
    "    def custom_loss(data, targets):\n",
    "        ''' Define custom loss function for weighted BCE on 'target' column '''\n",
    "        bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n",
    "        bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n",
    "        return (bce_loss_1 * loss_weight) + bce_loss_2\n",
    "\n",
    "    #Inference No training\n",
    "    #print(x_train_padded.shape)\n",
    "    print(x_test_padded.shape)\n",
    "    #print(y_train_torch.shape)\n",
    "    \n",
    "    rnn_results = []\n",
    "    for i in range(NUM_MODELS):\n",
    "        MODEL_DIR = paths[i]\n",
    "        \n",
    "        model_dict = {'fold0_models': ['0_model1_0_epoch.bin','0_model1_1_epoch.bin','0_model1_2_epoch.bin','0_model1_3_epoch.bin'], \n",
    "              'fold1_models': ['1_model1_0_epoch.bin','1_model1_1_epoch.bin','1_model1_2_epoch.bin','1_model1_3_epoch.bin'],\n",
    "              'fold2_models': ['2_model1_0_epoch.bin','2_model1_1_epoch.bin','2_model1_2_epoch.bin','2_model1_3_epoch.bin'],\n",
    "              'fold3_models': ['3_model1_0_epoch.bin','3_model1_1_epoch.bin','3_model1_2_epoch.bin','3_model1_3_epoch.bin'],\n",
    "              'fold4_models': ['4_model1_0_epoch.bin','4_model1_1_epoch.bin','4_model1_2_epoch.bin','4_model1_3_epoch.bin'] }\n",
    "\n",
    "        SPLITS = 5\n",
    "        EPOCHS = 4\n",
    "        OUTPUT_DIM = 7\n",
    "        batch_size = 512\n",
    "\n",
    "        test_dataset = data.TensorDataset(x_test_padded, test_lengths)\n",
    "        test_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n",
    "        test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "        checkpoint_weights = [2 ** epoch for epoch in range(EPOCHS)]\n",
    "\n",
    "        test_preds_all_folds = []\n",
    "        \n",
    "        #average predictions over 4 epochs per fold\n",
    "        for fld in model_dict.keys():    \n",
    "            test_preds = np.zeros((len(test_dataset),OUTPUT_DIM))\n",
    "            test_preds_all_epochs = []\n",
    "            for e in range(EPOCHS):\n",
    "                #print(model_dict[fld][e])\n",
    "                dict_load = torch.load(MODEL_DIR + model_dict[fld][e] )\n",
    "                dict_load['embedding.weight'] = torch.from_numpy(embedding_matrix)\n",
    "                m2 = NeuralNet(embedding_matrix,OUTPUT_DIM-1).cuda()  \n",
    "                m2.load_state_dict(dict_load)\n",
    "                m2.to('cuda')\n",
    "                for param in m2.parameters():\n",
    "                    param.requires_grad = False\n",
    "                m2.eval()\n",
    "\n",
    "                for i, x_batch in enumerate(test_loader):\n",
    "                    pred = m2(x_batch[0].to('cuda'))\n",
    "                    #print(pred.shape)\n",
    "                    test_preds[i * batch_size:(i + 1) * batch_size] = sigmoid(pred.detach().cpu().squeeze().numpy()) #.argmax(axis=-1)\n",
    "\n",
    "                #print(test_preds.shape)\n",
    "                test_preds_all_epochs.append(test_preds)\n",
    "\n",
    "            test_preds_all_folds.append(np.average(test_preds_all_epochs, weights=checkpoint_weights, axis=0))\n",
    "\n",
    "        test_pred_lstm = np.mean(test_preds_all_folds, axis=0)[:, 0]\n",
    "        rnn_results.append(test_pred_lstm) \n",
    "    \n",
    "    #delete everything except rnn_results\n",
    "    del train,test, symbols_to_isolate, symbols_to_delete,  y_aux_train, \n",
    "    del subgroup_bool_train, subgroup_negative_mask, toxic_bool_train, weights, loss_weight, \n",
    "    del embedding_matrix,  x_test_padded\n",
    "    #y_train_torch, x_train_padded, y_train,\n",
    "    del m2, test_dataset, test_collator, test_loader, test_preds_all_folds, test_pred_lstm, test_preds_all_epochs, \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return rnn_results #np.mean(rnn_results,axis=0)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = '../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'\n",
    "TEST_CSV = '../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv'\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)  \n",
    "test = pd.read_csv(TEST_CSV) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_model(paths, train, test):\n",
    "    \n",
    "    NUM_MODELS = len(paths)\n",
    "\n",
    "    GPT2_MODEL_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/'\n",
    "    TOKENIZER_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/'\n",
    "    GPT2_CONFIG_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/config.json'\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 220\n",
    "    SEED = 1234\n",
    "    BATCH_SIZE = 32\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    def convert_lines(example, max_seq_length,tokenizer):\n",
    "        max_seq_length -=2\n",
    "        all_tokens = []\n",
    "        longer = 0\n",
    "        #for text in tqdm_notebook(example):\n",
    "        for text in example:\n",
    "            tokens_a = tokenizer.tokenize(text)\n",
    "            if len(tokens_a)>max_seq_length:\n",
    "                tokens_a = tokens_a[:max_seq_length]\n",
    "                longer += 1\n",
    "            one_token = tokenizer.convert_tokens_to_ids(tokens_a)+[0] * (max_seq_length - len(tokens_a))\n",
    "            all_tokens.append(one_token)\n",
    "        print(longer)\n",
    "        return np.array(all_tokens)\n",
    "    \n",
    "    gpt2_config = GPT2Config(GPT2_CONFIG_PATH)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "    \n",
    "    test['comment_text'] = test['comment_text'].astype(str) \n",
    "    test['id1'] = np.arange(len(test))\n",
    "    \n",
    "    X_test = convert_lines(test[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "    y_columns = ['target', 'weights', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n",
    "    \n",
    "    gpt2_preds = []\n",
    "    for k in range(NUM_MODELS):\n",
    "        model = GPT2ClassificationHeadModel.from_pretrained(GPT2_MODEL_PATH, clf_dropout=0.2, n_class=len(y_columns))\n",
    "        model.load_state_dict(torch.load(paths[k]))\n",
    "        model.to(device)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.eval()\n",
    "    \n",
    "        test_preds = np.zeros((len(X_test)))\n",
    "        \n",
    "        \n",
    "        test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long),\n",
    "                                                  torch.tensor(test['id1'],dtype=torch.long))\n",
    "\n",
    "        ran_sampler = torch.utils.data.RandomSampler(test_dataset)\n",
    "        len_sampler = LenMatchBatchSampler(ran_sampler, batch_size=32, drop_last=False)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset,batch_sampler=len_sampler)\n",
    "\n",
    "        #test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n",
    "        #test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "        #tk0 = tqdm(test_loader)\n",
    "        tk0 = test_loader\n",
    "        for i, x_batch in enumerate(tk0):\n",
    "            tsrs = trim_tensors(x_batch)\n",
    "            b_input_ids, ids  = tuple(t.to(device) for t in tsrs)\n",
    "            pred = model(b_input_ids.to(device))\n",
    "            #test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "            test_preds[ids.detach().cpu().numpy()] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "    \n",
    "        test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n",
    "        \n",
    "        gpt2_preds.append(test_pred)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    del tokenizer, test, X_test\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return gpt2_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gpt2_model(paths, train, test):\n",
    "#     NUM_MODELS = len(paths)\n",
    "\n",
    "#     GPT2_MODEL_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/'\n",
    "#     TOKENIZER_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/'\n",
    "#     GPT2_CONFIG_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/config.json'\n",
    "\n",
    "#     MAX_SEQUENCE_LENGTH = 220\n",
    "#     SEED = 1234\n",
    "#     BATCH_SIZE = 32\n",
    "#     np.random.seed(SEED)\n",
    "#     torch.manual_seed(SEED)\n",
    "#     torch.cuda.manual_seed(SEED)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "#     def convert_lines(example, max_seq_length,tokenizer):\n",
    "#         max_seq_length -=2\n",
    "#         all_tokens = []\n",
    "#         longer = 0\n",
    "#         for text in tqdm_notebook(example):\n",
    "#             tokens_a = tokenizer.tokenize(text)\n",
    "#             if len(tokens_a)>max_seq_length:\n",
    "#                 tokens_a = tokens_a[:max_seq_length]\n",
    "#                 longer += 1\n",
    "#             one_token = tokenizer.convert_tokens_to_ids(tokens_a)+[0] * (max_seq_length - len(tokens_a))\n",
    "#             all_tokens.append(one_token)\n",
    "#         print(longer)\n",
    "#         return np.array(all_tokens)\n",
    "    \n",
    "#     gpt2_config = GPT2Config(GPT2_CONFIG_PATH)\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "    \n",
    "#     test['comment_text'] = test['comment_text'].astype(str) \n",
    "#     X_test = convert_lines(test[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "#     y_columns = ['target', 'weights', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n",
    "\n",
    "#     gpt2_preds = []\n",
    "#     for i in range(NUM_MODELS):\n",
    "#         model = GPT2ClassificationHeadModel.from_pretrained(GPT2_MODEL_PATH, clf_dropout=0.2, n_class=len(y_columns))\n",
    "#         model.load_state_dict(torch.load(paths[i]))\n",
    "#         model.to(device)\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         model.eval()\n",
    "    \n",
    "#         test_preds = np.zeros((len(X_test)))\n",
    "#         test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n",
    "#         test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "#         tk0 = tqdm(test_loader)\n",
    "#         for i, (x_batch,) in enumerate(tk0):\n",
    "#             pred = model(x_batch.to(device))\n",
    "#             test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "\n",
    "#         test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n",
    "        \n",
    "#         gpt2_preds.append(test_pred)\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "#     del tokenizer, test, X_test\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     return gpt2_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BERT Base...\n",
      "End BERT Base, time taken (seconds):  3386.538088321686\n"
     ]
    }
   ],
   "source": [
    "# ########################### Bert Large ###########################\n",
    "# BERT_LARGE_MODEL_DIR_BASE = '../input/tf-large-bert-inference-2/1560187254-20190610t173847z-001'\n",
    "# BERT_LARGE_MODEL = '/1560187254/'\n",
    "# start = time.time()\n",
    "# print(\"Starting BERT large...\")\n",
    "# bert_large_preds = bert_large(BERT_LARGE_MODEL_DIR_BASE, BERT_LARGE_MODEL, test)\n",
    "# del bert_large\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# #following required for bert large - GPU memory doesn't get released.\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "# print(\"End BERT large, time taken (seconds): \", time.time()-start)\n",
    "# ########################### Bert Large ###########################\n",
    "\n",
    "# # ########################### Bert Base ###########################\n",
    "BERT_BASE_MODEL_PATHS = ['../input/pytorch-bert-base-inference/bert_pytorch_mymodel.bin']  \n",
    "\n",
    "BERT_BASE_MODEL_PATHS = ['../input/bert-base-5-models/bert_model5folds/bert_model5folds/bert_pytorch_mymodel_fold1.bin',\n",
    "                        '../input/bert-base-5-models/bert_model5folds/bert_model5folds/bert_pytorch_mymodel_fold2.bin',\n",
    "                        '../input/bert-base-5-models/bert_model5folds/bert_model5folds/bert_pytorch_mymodel_fold3.bin',\n",
    "                        '../input/bert-base-5-models/bert_model5folds/bert_model5folds/bert_pytorch_mymodel_fold4.bin',\n",
    "                        '../input/bert-base-5-models/bert_model5folds/bert_model5folds/bert_pytorch_mymodel_fold5.bin',\n",
    "                        \n",
    "                        '../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold0.bin',\n",
    "                        '../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold1.bin',                         \n",
    "                        '../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold2.bin',\n",
    "                        '../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold3.bin',                         \n",
    "                        '../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold4.bin'                         \n",
    "                        ]\n",
    "\n",
    "\n",
    "#BERT_BASE_MODEL_PATHS = ['../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold1.bin']\n",
    "\n",
    "start = time.time()\n",
    "print(\"Starting BERT Base...\")\n",
    "bert_base_preds = bert_base(BERT_BASE_MODEL_PATHS,train,test)\n",
    "del bert_base\n",
    "gc.collect()\n",
    "print(\"End BERT Base, time taken (seconds): \", time.time()-start)\n",
    "# ########################### Bert Base ###########################\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_base_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPT2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../input/jigsawdatasets/pytorch-pretrained-gpt2/pytorch-pretrained-gpt2/pytorch_pretrained_bert/tokenization_gpt2.py:146: ResourceWarning: unclosed file <_io.TextIOWrapper name='../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/vocab.json' mode='r' encoding='UTF-8'>\n",
      "  self.encoder = json.load(open(vocab_file))\n",
      "../input/jigsawdatasets/pytorch-pretrained-gpt2/pytorch-pretrained-gpt2/pytorch_pretrained_bert/tokenization_gpt2.py:151: ResourceWarning: unclosed file <_io.TextIOWrapper name='../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/merges.txt' mode='r' encoding='utf-8'>\n",
      "  bpe_data = open(merges_file, encoding='utf-8').read().split('\\n')[1:-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n",
      "End GPT2, time taken (seconds):  1771.2205953598022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################## GPT2  ###########################\n",
    "GPT2_MODEL_PATHS = ['../input/gpt2-models-for-ensemble/gpt2_pytorch_mymodel_fold0.bin',\n",
    "                   '../input/gpt2-models-for-ensemble/gpt2_pytorch_mymodel_fold1.bin',\n",
    "                   '../input/gpt2-models-for-ensemble/gpt2_pytorch_mymodel_fold2.bin',\n",
    "                   '../input/gpt2-models-for-ensemble/gpt2_pytorch_mymodel_fold3.bin',\n",
    "                   '../input/gpt2-models-for-ensemble/gpt2_pytorch_mymodel_fold4.bin']\n",
    "start = time.time()\n",
    "print(\"Starting GPT2...\")\n",
    "gpt2_preds = gpt2_model(GPT2_MODEL_PATHS, train, test)\n",
    "del gpt2_model\n",
    "gc.collect()\n",
    "print(\"End GPT2, time taken (seconds): \", time.time()-start)\n",
    "########################## GPT2  ###########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488836\n",
      "n unknown words (crawl):  148789\n",
      "n unknown words (glove):  152310\n",
      "torch.Size([97320, 300])\n",
      "End RNN, time taken (seconds):  1295.441802740097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ########################### LSTM  ###########################\n",
    "RNN_PATHS = ['../input/jigsaw-model3/', '../input/rnn-for-ensemble/rnn/']\n",
    "start = time.time()\n",
    "print(\"Starting RNN...\")\n",
    "rnn_preds = rnn_lstm(RNN_PATHS, train, test)\n",
    "del rnn_lstm\n",
    "gc.collect()\n",
    "print(\"End RNN, time taken (seconds): \", time.time()-start)\n",
    "########################### LSTM  ###########################\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = (0.35*np.mean(bert_base_preds,axis=0)  + 0.35*bert_large_preds[:,0]  + 0.3*np.mean(rnn_preds,axis=0))\n",
    "\n",
    "#predictions = (np.mean(bert_base_preds,axis=0)) #+ np.mean(rnn_preds,axis=0))/2 #np.mean(gpt2_preds,axis=0)\n",
    "\n",
    "# predictions = np.mean(gpt2_preds,axis=0) *23/101\n",
    "# predictions += np.mean(bert_base_preds,axis=0)*48/101 \n",
    "# predictions +=  np.mean(rnn_preds,axis=0)  * 30/101\n",
    "\n",
    "#predictions = (np.mean(bert_base_preds,axis=0))\n",
    "\n",
    "#28 bert + 27 bert + 16 rnn + 19 gpt2 + 11 rnn) \n",
    "predictions = np.mean(bert_base_preds[0:5],axis=0)*28/101.0 + np.mean(bert_base_preds[5:],axis=0)*27/101.0\n",
    "predictions += rnn_preds[0]*16/101.0 + rnn_preds[1]*11/101.0\n",
    "predictions += np.mean(gpt2_preds,axis=0)*19/101.0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction':  predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.002139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.002047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.000771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.989459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7000005</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7000006</td>\n",
       "      <td>0.001023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7000007</td>\n",
       "      <td>0.002543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7000008</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7000009</td>\n",
       "      <td>0.006536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7000010</td>\n",
       "      <td>0.000760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7000011</td>\n",
       "      <td>0.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7000012</td>\n",
       "      <td>0.000158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7000013</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7000014</td>\n",
       "      <td>0.000568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7000015</td>\n",
       "      <td>0.001716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7000016</td>\n",
       "      <td>0.037136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7000017</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7000018</td>\n",
       "      <td>0.260599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7000019</td>\n",
       "      <td>0.013342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7000020</td>\n",
       "      <td>0.007234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7000021</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7000022</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7000023</td>\n",
       "      <td>0.481425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7000024</td>\n",
       "      <td>0.920243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7000025</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7000026</td>\n",
       "      <td>0.046282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7000027</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7000028</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7000029</td>\n",
       "      <td>0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97290</th>\n",
       "      <td>7097290</td>\n",
       "      <td>0.001858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97291</th>\n",
       "      <td>7097291</td>\n",
       "      <td>0.338689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97292</th>\n",
       "      <td>7097292</td>\n",
       "      <td>0.125660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97293</th>\n",
       "      <td>7097293</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97294</th>\n",
       "      <td>7097294</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97295</th>\n",
       "      <td>7097295</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97296</th>\n",
       "      <td>7097296</td>\n",
       "      <td>0.001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97297</th>\n",
       "      <td>7097297</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97298</th>\n",
       "      <td>7097298</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97299</th>\n",
       "      <td>7097299</td>\n",
       "      <td>0.011112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97300</th>\n",
       "      <td>7097300</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97301</th>\n",
       "      <td>7097301</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97302</th>\n",
       "      <td>7097302</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97303</th>\n",
       "      <td>7097303</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97304</th>\n",
       "      <td>7097304</td>\n",
       "      <td>0.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97305</th>\n",
       "      <td>7097305</td>\n",
       "      <td>0.057545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97306</th>\n",
       "      <td>7097306</td>\n",
       "      <td>0.000444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97307</th>\n",
       "      <td>7097307</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97308</th>\n",
       "      <td>7097308</td>\n",
       "      <td>0.006310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97309</th>\n",
       "      <td>7097309</td>\n",
       "      <td>0.002865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97310</th>\n",
       "      <td>7097310</td>\n",
       "      <td>0.005392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97311</th>\n",
       "      <td>7097311</td>\n",
       "      <td>0.004360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97312</th>\n",
       "      <td>7097312</td>\n",
       "      <td>0.107040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97313</th>\n",
       "      <td>7097313</td>\n",
       "      <td>0.051441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97314</th>\n",
       "      <td>7097314</td>\n",
       "      <td>0.000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97315</th>\n",
       "      <td>7097315</td>\n",
       "      <td>0.002659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97316</th>\n",
       "      <td>7097316</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97317</th>\n",
       "      <td>7097317</td>\n",
       "      <td>0.016423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97318</th>\n",
       "      <td>7097318</td>\n",
       "      <td>0.050484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97319</th>\n",
       "      <td>7097319</td>\n",
       "      <td>0.000127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97320 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prediction\n",
       "0      7000000    0.002139\n",
       "1      7000001    0.000075\n",
       "2      7000002    0.002047\n",
       "3      7000003    0.000771\n",
       "4      7000004    0.989459\n",
       "5      7000005    0.000083\n",
       "6      7000006    0.001023\n",
       "7      7000007    0.002543\n",
       "8      7000008    0.016949\n",
       "9      7000009    0.006536\n",
       "10     7000010    0.000760\n",
       "11     7000011    0.093900\n",
       "12     7000012    0.000158\n",
       "13     7000013    0.000130\n",
       "14     7000014    0.000568\n",
       "15     7000015    0.001716\n",
       "16     7000016    0.037136\n",
       "17     7000017    0.000242\n",
       "18     7000018    0.260599\n",
       "19     7000019    0.013342\n",
       "20     7000020    0.007234\n",
       "21     7000021    0.000253\n",
       "22     7000022    0.000093\n",
       "23     7000023    0.481425\n",
       "24     7000024    0.920243\n",
       "25     7000025    0.000336\n",
       "26     7000026    0.046282\n",
       "27     7000027    0.000137\n",
       "28     7000028    0.000210\n",
       "29     7000029    0.001345\n",
       "...        ...         ...\n",
       "97290  7097290    0.001858\n",
       "97291  7097291    0.338689\n",
       "97292  7097292    0.125660\n",
       "97293  7097293    0.000074\n",
       "97294  7097294    0.000188\n",
       "97295  7097295    0.000193\n",
       "97296  7097296    0.001025\n",
       "97297  7097297    0.000177\n",
       "97298  7097298    0.000475\n",
       "97299  7097299    0.011112\n",
       "97300  7097300    0.000193\n",
       "97301  7097301    0.001572\n",
       "97302  7097302    0.000107\n",
       "97303  7097303    0.000175\n",
       "97304  7097304    0.108400\n",
       "97305  7097305    0.057545\n",
       "97306  7097306    0.000444\n",
       "97307  7097307    0.000109\n",
       "97308  7097308    0.006310\n",
       "97309  7097309    0.002865\n",
       "97310  7097310    0.005392\n",
       "97311  7097311    0.004360\n",
       "97312  7097312    0.107040\n",
       "97313  7097313    0.051441\n",
       "97314  7097314    0.000724\n",
       "97315  7097315    0.002659\n",
       "97316  7097316    0.000071\n",
       "97317  7097317    0.016423\n",
       "97318  7097318    0.050484\n",
       "97319  7097319    0.000127\n",
       "\n",
       "[97320 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame([np.mean(bert_base_preds,axis=0),np.mean(rnn_preds,axis=0),np.mean(gpt2_preds,axis=0)]).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
