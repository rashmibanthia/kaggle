{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Berts + GPT2 + 2RNN (In the ratio : 28 bert + 27 bert + 16 rnn + 19 gpt2 + 11 rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import fastai\n",
    "from fastai.train import Learner\n",
    "from fastai.train import DataBunch\n",
    "from fastai.callbacks import *\n",
    "from fastai.basic_data import DatasetType\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import collections\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "#from tqdm import tqdm, tqdm_notebook\n",
    "#import fastprogress\n",
    "#from fastprogress import force_console_behavior\n",
    "#from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "#tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_dir = '../input/jigsawdatasets/pytorch-pretrained-gpt2/pytorch-pretrained-gpt2/'\n",
    "#sys.path.append(package_dir)\n",
    "sys.path.insert(1,package_dir)\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "from pytorch_pretrained_bert import GPT2Tokenizer, GPT2ClassificationHeadModel\n",
    "from pytorch_pretrained_bert import OpenAIAdam\n",
    "from pytorch_pretrained_bert import GPT2Config\n",
    "\n",
    "# # disable progress bars when submitting\n",
    "# def is_interactive():\n",
    "#    return 'SHLVL' not in os.environ\n",
    "\n",
    "# if not is_interactive():\n",
    "#     def nop(it, *a, **k):\n",
    "#         return it\n",
    "#     tqdm = nop\n",
    "#     fastprogress.fastprogress.NO_BAR = True\n",
    "#     master_bar, progress_bar = force_console_behavior()\n",
    "#     fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package_dir = '../input/jigsawdatasets/pytorch-pretrained-gpt2/pytorch_pretrained_gpt2/pytorch_pretrained_bert'\n",
    "# sys.path.insert(1,package_dir)\n",
    "\n",
    "# from pytorch_pretrained_bert  import GPT2Tokenizer, GPT2ClassificationHeadModel\n",
    "# # from pytorch_pretrained_bert import OpenAIAdam\n",
    "# # from pytorch_pretrained_bert import GPT2Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "device = torch.device('cuda')\n",
    "\n",
    "    \n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"Trump's\" : 'trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n",
    "    'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not',\n",
    "    'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n",
    "    'can;t': 'cannot','can;t;ve': 'cannot have',\n",
    "    'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n",
    "    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n",
    "    'couldn;t;ve': 'could not have','couldn´t': 'could not',\n",
    "    'couldn´t´ve': 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could´ve': 'could have',\n",
    "    'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n",
    "    'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n",
    "    'doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n",
    "    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n",
    "    'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not','hasn’t': 'has not',\n",
    "    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not','haven’t': 'have not',\"he'd\": 'he would',\n",
    "    \"he'd've\": 'he would have',\"he'll\": 'he will',\n",
    "    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n",
    "    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have','he´ll': 'he will',\n",
    "    'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n",
    "    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n",
    "    'how;s': 'how is','how´d': 'how did','how´ll': 'how will','how´s': 'how is','how’d': 'how did','how’ll': 'how will',\n",
    "    'how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n",
    "    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n",
    "    'isn,t': 'is not','isn;t': 'is not','isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n",
    "    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it´d': 'it would','it´ll': 'it will','it´s': 'it is',\n",
    "    'it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n",
    "    'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n",
    "    'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us',\n",
    "    'let’s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n",
    "    'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn´t': 'might not',\n",
    "    'mightn’t': 'might not','might´ve': 'might have','might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n",
    "    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not','must´ve': 'must have',\n",
    "    'must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n",
    "    'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n",
    "    'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not','sha´n´t': 'shall not','sha’n’t': 'shall not',\n",
    "    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n",
    "    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she´d': 'she would','she´ll': 'she will',\n",
    "    'she´s': 'she is','she’d': 'she would','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n",
    "    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have',\n",
    "    'should’ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n",
    "    'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\"there'd\": 'there had',\n",
    "    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n",
    "    'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n",
    "    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n",
    "    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n",
    "    'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are','they´ve': 'they have','they’d': 'they would','they’ll': 'they will',\n",
    "    'they’re': 'they are','they’ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not',\n",
    "    'wasn’t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n",
    "    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n",
    "    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not','we´d': 'we would','we´ll': 'we will',\n",
    "    'we´re': 'we are','we´ve': 'we have','we’d': 'we would','we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n",
    "    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n",
    "    'what;s': 'what is','what;ve': 'what have','what´ll': 'what will',\n",
    "    'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will','what’re': 'what are','what’s': 'what is',\n",
    "    'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n",
    "    'where;s': 'where is','where´d': 'where did','where´s': 'where is','where’d': 'where did','where’s': 'where is',\n",
    "    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n",
    "    'who´ll': 'who will','who´s': 'who is','who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n",
    "    'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn´t': 'would not',\n",
    "    'wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n",
    "    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n",
    "    'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would','you’ll': 'you will','you’re': 'you are',\n",
    "    '´cause': 'because','’cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n",
    "    \"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n",
    "    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n",
    "    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n",
    "    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n",
    "    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I',\n",
    "    'ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at','…and':'and','civilbeat':'civil beat',\\\n",
    "    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\\\n",
    "    'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','Sᴛᴀʀᴛ':'start','gubmit':'submit','CO₂':'carbon dioxide','ғɪʀsᴛ':'first',\\\n",
    "    'ᴇɴᴅ':'end','ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra',\\\n",
    "    'Gʀᴇᴀᴛ':'great','sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\\\n",
    "    'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','Mᴀᴋᴇ':'make',\\\n",
    "    'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','financialpost':'financial post', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'Maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', 'ɴᴇᴇᴅ':' need ',\n",
    "    'ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'Sᴛaʀᴛ':' start', 'SHOPO':'shop',\n",
    "    }\n",
    "mispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','Gʀᴇat':'great','ʙᴏᴛtoᴍ':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yᴏᴜ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','ᴀ':'a', '😉':'wink','😂':'joy','😀':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\n",
    "\n",
    "special_punc_mappings = {\"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '”': '\"', \"″\": '\"', '“': '\"', '•': '.', '−': '-',\n",
    "                         \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','،':'','„':'',\n",
    "                         '…': ' ... ', '\\ufeff': ''}\n",
    "\n",
    "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "\n",
    "rare_words_mapping = {' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA','u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA', ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',\n",
    "                      ' U.s ': 'USA', ' u.S ': ' USA ', 'fu.k': 'fuck', 'U.K.': 'UK', ' u.k ': ' UK ',' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n",
    "                      'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n",
    "                      'Ra apist': 'Rapist', '2fifth': 'twenty fifth', '2third': 'twenty third','2nineth': 'twenty nineth', '2fourth': 'twenty fourth', '#metoo': 'MeToo',\n",
    "                      'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n",
    "                      'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin','fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n",
    "                      'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n",
    "                      'culturr': 'culture','weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n",
    "                      'indans': 'indians', 'mastuburate': 'masturbate', 'f**k': 'fuck', 'F**k': 'fuck', 'F**K': 'fuck',\n",
    "                      ' u r ': ' you are ', ' u ': ' you ', '操你妈': 'fuck your mother', 'e.g.': 'for example',\n",
    "                      'i.e.': 'in other words', '...': '.', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n",
    "                      'f***': 'fuck', 'f**': 'fuc', 'F***': 'fuck', 'F**': 'fuc','a****': 'assho', 'a**': 'ass', 'h***': 'hole', 'A****': 'assho', 'A**': 'ass', 'H***': 'hole',\n",
    "                      's***': 'shit', 's**': 'shi', 'S***': 'shit', 'S**': 'shi', 'Sh**': 'shit',\n",
    "                      'p****': 'pussy', 'p*ssy': 'pussy', 'P****': 'pussy','p***': 'porn', 'p*rn': 'porn', 'P***': 'porn',\n",
    "                      'st*up*id': 'stupid','d***': 'dick', 'di**': 'dick', 'h*ck': 'hack',\n",
    "                      'b*tch': 'bitch', 'bi*ch': 'bitch', 'bit*h': 'bitch', 'bitc*': 'bitch', 'b****': 'bitch',\n",
    "                      'b***': 'bitc', 'b**': 'bit', 'b*ll': 'bull'\n",
    "                      }\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "\n",
    "def pre_clean_rare_words(text):\n",
    "    for rare_word in rare_words_mapping:\n",
    "        if rare_word in text:\n",
    "            text = text.replace(rare_word, rare_words_mapping[rare_word])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LenMatchBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __iter__(self):\n",
    "        buckets = [[]] * 100\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            count_zeros = torch.sum(self.sampler.data_source[idx][0] == 0)\n",
    "            count_zeros = int(count_zeros / 64) \n",
    "            if len(buckets[count_zeros]) == 0:  buckets[count_zeros] = []\n",
    "\n",
    "            buckets[count_zeros].append(idx)\n",
    "\n",
    "            if len(buckets[count_zeros]) == self.batch_size:\n",
    "                batch = list(buckets[count_zeros])\n",
    "                #print(\"batch:\",batch)\n",
    "                yield batch\n",
    "                yielded += 1\n",
    "                buckets[count_zeros] = []\n",
    "\n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yielded += 1\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yielded += 1\n",
    "            yield batch\n",
    "\n",
    "        assert len(self) == yielded, \"produced an inccorect number of batches. expected %i, but yielded %i\" %(len(self), yielded)\n",
    "\n",
    "def trim_tensors(tsrs):\n",
    "    max_len = torch.max(torch.sum( (tsrs[0] != 0  ), 1))\n",
    "    tsrs1 = []\n",
    "    if max_len > 2:         \n",
    "        tsrs = [tsr[:, :max_len] if i!=1 else tsr for i,tsr in enumerate(tsrs) ]\n",
    "        \n",
    "    return tsrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_base(paths, train, test):\n",
    "    \n",
    "    NUM_MODELS = len(paths)\n",
    "    def convert_lines(example, max_seq_length,tokenizer):\n",
    "        max_seq_length -=2\n",
    "        all_tokens = []\n",
    "        longer = 0\n",
    "        #for text in tqdm(example):\n",
    "        for text in example:\n",
    "            tokens_a = tokenizer.tokenize(text)\n",
    "            if len(tokens_a)>max_seq_length:\n",
    "                tokens_a = tokens_a[:max_seq_length]\n",
    "                longer += 1\n",
    "            one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "            all_tokens.append(one_token)\n",
    "        return np.array(all_tokens)\n",
    "    \n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH = 220 + 2\n",
    "    SEED = 1234\n",
    "    BATCH_SIZE = 32\n",
    "    BERT_MODEL_PATH = '../input/jigsawdatasets/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    #All I need from bert_inference  is bert_config.json\n",
    "    #bert_config = BertConfig('../input/jigsawdatasets/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_config.json')\n",
    "    \n",
    "    bert_config = BertConfig(vocab_size_or_config_json_file=30522, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072,hidden_act=\"gelu\",\n",
    "                        hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512, type_vocab_size=2,initializer_range=0.02 ) \n",
    "#                 layer_norm_eps=1e-12)\n",
    "                \n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "\n",
    "    test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n",
    "    test_df['id1'] = np.arange(len(test_df))\n",
    "    test_df['comment_text'] = test_df['comment_text'].astype(str) \n",
    "    test_df['comment_text'] = test_df['comment_text'].apply(lambda x: correct_contraction(x, contraction_mapping))\n",
    "    X_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "    \n",
    "    bert_base_preds = []\n",
    "    for k in range(NUM_MODELS):\n",
    "        model = BertForSequenceClassification(bert_config, num_labels=8)\n",
    "        #model.load_state_dict(torch.load(\"../input/bertmodel-jigsaw/bert_pytorch_mymodel.bin\"))\n",
    "        model.load_state_dict(torch.load(paths[k]))\n",
    "\n",
    "        model.to(device)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.eval()\n",
    "\n",
    "        test_preds = np.zeros((len(X_test)))\n",
    "        \n",
    "        test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long),\n",
    "                                              torch.tensor(test_df['id1'],dtype=torch.long),\n",
    "                                              torch.from_numpy(np.array(X_test>0, dtype=np.uint8)),\n",
    "                                              torch.zeros(X_test.shape),  torch.zeros(X_test.shape))\n",
    "\n",
    "        ran_sampler = torch.utils.data.RandomSampler(test_dataset)\n",
    "        len_sampler = LenMatchBatchSampler(ran_sampler, batch_size=32, drop_last=False)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset,batch_sampler=len_sampler)\n",
    "        \n",
    "#         test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n",
    "#         test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "        #tk0 = tqdm(test_loader)\n",
    "        tk0 = test_loader\n",
    "        for i, x_batch in enumerate(tk0):\n",
    "            tsrs = trim_tensors(x_batch)\n",
    "            #pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n",
    "            #test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "            b_input_ids, ids, b_input_mask, b_segment_ids,b_label = tuple(t.to(device) for t in tsrs)\n",
    "            pred = model(b_input_ids,attention_mask=b_input_mask, labels=None)    \n",
    "            test_preds[ids.detach().cpu().numpy()] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "\n",
    "\n",
    "        test_pred_base = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n",
    "\n",
    "        bert_base_preds.append(test_pred_base)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return bert_base_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bert_large(large_model_dir_base, large_model_dir_path,test):\n",
    "#     # This function is not used by this file but is still used by the Colab and\n",
    "#     # people who depend on it.\n",
    "#     def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "#                                      tokenizer):\n",
    "#         \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "#         features = []\n",
    "#         for (ex_index, example) in enumerate(examples):\n",
    "#             if ex_index % 10000 == 0:\n",
    "#                 tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "#             feature = convert_single_example(ex_index, example, label_list,\n",
    "#                                      max_seq_length, tokenizer)\n",
    "            \n",
    "#             features.append(feature)            \n",
    "#         return features\n",
    "    \n",
    "    \n",
    "\n",
    "#     def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "#                                tokenizer):\n",
    "#         \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "\n",
    "#         if isinstance(example, PaddingInputExample):\n",
    "#             return InputFeatures(\n",
    "#                 input_ids=[0] * max_seq_length,\n",
    "#                 input_mask=[0] * max_seq_length,\n",
    "#                 segment_ids=[0] * max_seq_length,\n",
    "#                 label_id=[0]*len(LABEL_COLUMNS),\n",
    "#                 is_real_example=False)\n",
    "\n",
    "#         label_map = {}\n",
    "#         for (i, label) in enumerate(label_list):\n",
    "#             label_map[label] = i\n",
    "\n",
    "#         tokens_a = tokenizer.tokenize(example.text_a)\n",
    "#         tokens_b = None\n",
    "#         if example.text_b:\n",
    "#             tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "#         if tokens_b:\n",
    "#             # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "#             # length is less than the specified length.\n",
    "#             # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "#             _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "#         else:\n",
    "#             # Account for [CLS] and [SEP] with \"- 2\"\n",
    "#             if len(tokens_a) > max_seq_length - 2:\n",
    "#                 tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "#         # The convention in BERT is:\n",
    "#         # (a) For sequence pairs:\n",
    "#         #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "#         #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "#         # (b) For single sequences:\n",
    "#         #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "#         #  type_ids: 0     0   0   0  0     0 0\n",
    "#         #\n",
    "#         # Where \"type_ids\" are used to indicate whether this is the first\n",
    "#         # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "#         # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "#         # embedding vector (and position vector). This is not *strictly* necessary\n",
    "#         # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "#         # it easier for the model to learn the concept of sequences.\n",
    "#         #\n",
    "#         # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "#         # used as the \"sentence vector\". Note that this only makes sense because\n",
    "#         # the entire model is fine-tuned.\n",
    "#         tokens = []\n",
    "#         segment_ids = []\n",
    "#         tokens.append(\"[CLS]\")\n",
    "#         segment_ids.append(0)\n",
    "#         for token in tokens_a:\n",
    "#             tokens.append(token)\n",
    "#             segment_ids.append(0)\n",
    "#         tokens.append(\"[SEP]\")\n",
    "#         segment_ids.append(0)\n",
    "\n",
    "#         if tokens_b:\n",
    "#             for token in tokens_b:\n",
    "#                 tokens.append(token)\n",
    "#                 segment_ids.append(1)\n",
    "#             tokens.append(\"[SEP]\")\n",
    "#             segment_ids.append(1)\n",
    "\n",
    "#         input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "#         # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "#         # tokens are attended to.\n",
    "#         input_mask = [1] * len(input_ids)\n",
    "\n",
    "#         # Zero-pad up to the sequence length.\n",
    "#         while len(input_ids) < max_seq_length:\n",
    "#             input_ids.append(0)\n",
    "#             input_mask.append(0)\n",
    "#             segment_ids.append(0)\n",
    "\n",
    "#         assert len(input_ids) == max_seq_length\n",
    "#         assert len(input_mask) == max_seq_length\n",
    "#         assert len(segment_ids) == max_seq_length\n",
    "\n",
    "#         #label_id = label_map[example.label]\n",
    "#         labels_ids = []\n",
    "#         #print(ex_index, example.label)\n",
    "#         for label in example.label:\n",
    "#             labels_ids.append(float(label))\n",
    "\n",
    "#         if ex_index < 5:\n",
    "#             tf.logging.info(\"*** Example ***\")\n",
    "#             tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "#             tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "#                 [tokenization.printable_text(x) for x in tokens]))\n",
    "#             tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#             tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#             tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "#             tf.logging.info(\"label: %s (id = %s)\" % (example.label, labels_ids))\n",
    "\n",
    "#         feature = InputFeatures(\n",
    "#                             input_ids=input_ids,\n",
    "#                             input_mask=input_mask,\n",
    "#                             segment_ids=segment_ids,\n",
    "#                             label_id=labels_ids,\n",
    "#                             is_real_example=True)\n",
    "#         return feature\n",
    "\n",
    "\n",
    "#     def file_based_convert_examples_to_features(\n",
    "#         examples, label_list, max_seq_length, tokenizer, output_file):\n",
    "#         \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
    "\n",
    "#         writer = tf.python_io.TFRecordWriter(output_file)\n",
    "#         def create_int_feature(values):\n",
    "#             f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "#             return f\n",
    "\n",
    "#         def create_float_feature(values):\n",
    "#             f = tf.train.Feature(float_list = tf.train.FloatList(value=list(values)))\n",
    "#             return f\n",
    "\n",
    "#         for (ex_index, example) in enumerate(examples):\n",
    "#             if ex_index % 10000 == 0:\n",
    "#                 tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "#             feature = convert_single_example(ex_index, example, label_list,\n",
    "#                                          max_seq_length, tokenizer)\n",
    "\n",
    "# #             def create_int_feature(values):\n",
    "# #                 f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "# #                 return f\n",
    "\n",
    "# #             def create_float_feature(values):\n",
    "# #                 f = tf.train.Feature(float_list = tf.train.FloatList(value=list(values)))\n",
    "# #                 return f\n",
    "\n",
    "#             features = collections.OrderedDict()\n",
    "#             features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "#             features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "#             features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "\n",
    "#             features[\"label_ids\"] = create_float_feature(feature.label_id)\n",
    "\n",
    "#             features[\"is_real_example\"] = create_int_feature(\n",
    "#                 [int(feature.is_real_example)])\n",
    "\n",
    "#             tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "#             writer.write(tf_example.SerializeToString())\n",
    "            \n",
    "#         writer.close()\n",
    "    \n",
    "#     #del test  \n",
    "#     gc.collect()\n",
    "    \n",
    "#     export_dir_base = '../input/tf-large-bert-inference-2/1560187254-20190610t173847z-001'\n",
    "#     export_dir_base = large_model_dir_base\n",
    "#     data_dir = '/kaggle/working'\n",
    "#     vocab_file = '../input/jigsawdatasets/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "\n",
    "#     from tensorflow.contrib import predictor\n",
    "\n",
    "#     predict_fn_e = predictor.from_saved_model(export_dir_base + large_model_dir_path)\n",
    "\n",
    "\n",
    "#     #test=pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "#     test['comment_text'] = test['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True).replace(r'\\t',  ' ', regex=True)\n",
    "\n",
    "#     test.to_csv( 'test.tsv', sep='\\t', index=False, header=True)\n",
    "\n",
    "#     processor = ColaProcessor()\n",
    "#     #label_list = processor.get_labels()\n",
    "#     label_list = [0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "\n",
    "\n",
    "#     predict_examples = processor.get_test_examples(data_dir)\n",
    "\n",
    "#     tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n",
    "\n",
    "\n",
    "#     max_seq_length = 222\n",
    "#     #num_actual_predict_examples = len(predict_examples)\n",
    "#     #predict_examples = predict_examples + predict_examples[0:24]\n",
    "\n",
    "#     i_features = convert_examples_to_features(predict_examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "\n",
    "#     k = len(i_features)\n",
    "#     input_ids_list = []\n",
    "#     input_mask_list = []\n",
    "#     label_ids_list = []\n",
    "#     segment_ids_list = []\n",
    "#     for i in range(k):\n",
    "#        input_ids_list.append(i_features[i].input_ids)\n",
    "#        input_mask_list.append(i_features[i].input_mask)\n",
    "#        #label_ids_list.append(i_features[i].label_id)\n",
    "#        segment_ids_list.append(i_features[i].segment_ids) \n",
    "\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     #prediction with batches \n",
    "\n",
    "# #    start = time.time()\n",
    "# #     print(\"--------------------------------------------------------\")\n",
    "# #     print(\"Starting infer ...\")\n",
    "# #     print(\"--------------------------------------------------------\")\n",
    "#     batch_size = 512 #1280 #1024\n",
    "\n",
    "\n",
    "\n",
    "#     k = (int)(np.ceil(len(predict_examples)/batch_size))\n",
    "#     #tq = tqdm_notebook(range(k))\n",
    "#     tq =  range(k) \n",
    "#     preds = np.array(np.zeros((len(predict_examples),8)))\n",
    "#     for i in tq:\n",
    "#     #    tmp = predict_fn_e({'input_ids': input_ids_list[i * batch_size:(i+1) * batch_size] , \n",
    "#     #                        'input_mask': input_mask_list[i * batch_size:(i+1) * batch_size], \n",
    "#     #                        #'label_ids': label_ids_list[i * batch_size:(i+1) * batch_size],\n",
    "#     #                        'segment_ids':segment_ids_list[i * batch_size:(i+1) * batch_size] } )['probabilities']\n",
    "#     #    print(tmp.shape)\n",
    "\n",
    "#         preds[i * batch_size:(i+1) * batch_size,:] = predict_fn_e({'input_ids': input_ids_list[i * batch_size:(i+1) * batch_size] , \n",
    "#                            'input_mask': input_mask_list[i * batch_size:(i+1) * batch_size], \n",
    "#                            #'label_ids': label_ids_list[i * batch_size:(i+1) * batch_size],\n",
    "#                            'segment_ids':segment_ids_list[i * batch_size:(i+1) * batch_size] } )['probabilities']\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     end = time.time()\n",
    "# #     print(\"--------------------------------------------------------\")\n",
    "# #     print(\"Infer complete in \", end - start, \" seconds\")\n",
    "# #     print(\"--------------------------------------------------------\")\n",
    "    \n",
    "#     del predict_fn_e,predict_examples, processor, input_ids_list, input_mask_list, segment_ids_list\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     tf.reset_default_graph()\n",
    "    \n",
    "#     return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_lstm(paths, train, test):\n",
    "    \n",
    "    NUM_MODELS = len(paths)\n",
    "    CRAWL_EMBEDDING_PATH = '../input/jigsawdatasets/crawl-300d-2M.pkl'\n",
    "    GLOVE_EMBEDDING_PATH = '../input/jigsawdatasets/glove.840B.300d.pkl'\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    def load_embeddings(path):\n",
    "        with open(path,'rb') as f:\n",
    "            emb_arr = pickle.load(f)\n",
    "        return emb_arr\n",
    "\n",
    "    def build_matrix(word_index, path):\n",
    "        embedding_index = load_embeddings(path)\n",
    "        embedding_matrix = np.zeros((max_features + 1, 300))\n",
    "        unknown_words = []\n",
    "\n",
    "        for word, i in word_index.items():\n",
    "            if i <= max_features:\n",
    "                try:\n",
    "                    embedding_matrix[i] = embedding_index[word]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        embedding_matrix[i] = embedding_index[word.lower()]\n",
    "                    except KeyError:\n",
    "                        try:\n",
    "                            embedding_matrix[i] = embedding_index[word.title()]\n",
    "                        except KeyError:\n",
    "                            unknown_words.append(word)\n",
    "        return embedding_matrix, unknown_words\n",
    "    \n",
    "    test['comment_text'] = test['comment_text'].astype(str).fillna(\"DUMMY_VALUE\")\n",
    "    \n",
    "    symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\n",
    "    symbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'\n",
    "\n",
    "    from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "    remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "    def handle_punctuation(x):\n",
    "        x = x.translate(remove_dict)\n",
    "        x = x.translate(isolate_dict)\n",
    "        return x\n",
    "\n",
    "    def handle_contractions(x):\n",
    "        x = tokenizer.tokenize(x)\n",
    "        return x\n",
    "    \n",
    "    def fix_quote(x):\n",
    "        x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "        x = ' '.join(x)\n",
    "        return x\n",
    "\n",
    "    def preprocess(x):\n",
    "        x = handle_punctuation(x)\n",
    "        x = handle_contractions(x)\n",
    "        x = fix_quote(x)\n",
    "\n",
    "        x = x.replace(\"n't\",'not')\n",
    "        x = x.replace(\"N'T\",'NOT')\n",
    "        x = x.replace('tRump','Trump')\n",
    "        x = x.replace(\"gov't\",\"government\")\n",
    "        x = x.replace(\"Gov't\",\"Government\")\n",
    "        x = x.replace(\"Twitler\",\"Twitter\")\n",
    "        return x\n",
    "    \n",
    "    #x_train = train['comment_text'].progress_apply(lambda x:preprocess(x))\n",
    "    x_train = train['comment_text'].apply(lambda x:preprocess(x))\n",
    "    y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "    x_test = test['comment_text'].apply(lambda x:preprocess(x))\n",
    "    #x_test = test['comment_text'].progress_apply(lambda x:preprocess(x))\n",
    "\n",
    "    TOXICITY_COLUMN = 'target'\n",
    "    identity_columns = ['asian', 'atheist',\n",
    "           'bisexual', 'black', 'buddhist', 'christian', 'female',\n",
    "           'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n",
    "           'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n",
    "           'muslim', 'other_disability', 'other_gender',\n",
    "           'other_race_or_ethnicity', 'other_religion',\n",
    "           'other_sexual_orientation', 'physical_disability',\n",
    "           'psychiatric_or_mental_illness', 'transgender', 'white']\n",
    "\n",
    "    subgroup_bool_train = train[identity_columns].fillna(0)>=0.5\n",
    "    toxic_bool_train = train[TOXICITY_COLUMN].fillna(0)>=0.5\n",
    "    subgroup_negative_mask = subgroup_bool_train.values.sum(axis=1).astype(bool) & ~toxic_bool_train\n",
    "    # Overall\n",
    "    weights = np.ones((len(train),))\n",
    "    # Subgroup negative\n",
    "    weights += subgroup_negative_mask\n",
    "    loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "    #y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n",
    "    \n",
    "    max_features = 400000\n",
    "    tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False, oov_token = 'UNK')\n",
    "    tokenizer.fit_on_texts(list(x_train)+list(x_test))\n",
    "    print(len(tokenizer.word_counts)) \n",
    "\n",
    "    crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "    print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "\n",
    "    glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "    print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "\n",
    "    max_features = max_features or len(tokenizer.word_index) + 1\n",
    "    embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "    \n",
    "    del crawl_matrix\n",
    "    del glove_matrix\n",
    "    gc.collect()\n",
    "\n",
    "    #x_train_torch = torch.tensor(x_train, dtype=torch.long)\n",
    "    #y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #x_train = tokenizer.texts_to_sequences(x_train)\n",
    "    x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "    #lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    "\n",
    "    maxlen = 300\n",
    "    #x_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))\n",
    "    #print(x_train_padded.shape)\n",
    "\n",
    "    # lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    "    test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n",
    "    x_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))\n",
    "    \n",
    "    del x_train, x_test, tokenizer\n",
    "    gc.collect()\n",
    "    \n",
    "    class SequenceBucketCollator():\n",
    "        def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "            self.choose_length = choose_length\n",
    "            self.sequence_index = sequence_index\n",
    "            self.length_index = length_index\n",
    "            self.label_index = label_index\n",
    "\n",
    "        def __call__(self, batch):\n",
    "            batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "\n",
    "            sequences = batch[self.sequence_index]\n",
    "            lengths = batch[self.length_index]\n",
    "\n",
    "            length = self.choose_length(lengths)\n",
    "            mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "            padded_sequences = sequences[:, mask]\n",
    "\n",
    "            batch[self.sequence_index] = padded_sequences\n",
    "\n",
    "            if self.label_index is not None:\n",
    "                return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "\n",
    "            return batch\n",
    "        \n",
    "    LSTM_UNITS = 128\n",
    "    DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    class SpatialDropout(nn.Dropout2d):\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "            x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "            x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "            x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "            x = x.squeeze(2)  # (N, T, K)\n",
    "            return x\n",
    "        \n",
    "    class NeuralNet(nn.Module):\n",
    "        def __init__(self, embedding_matrix, num_aux_targets):\n",
    "            super(NeuralNet, self).__init__()\n",
    "            embed_size = embedding_matrix.shape[1]\n",
    "\n",
    "            self.embedding = nn.Embedding(max_features, embed_size)\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            self.embedding_dropout = SpatialDropout(0.2)\n",
    "\n",
    "            self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "            self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "\n",
    "            self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "            self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "\n",
    "            self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "            self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "\n",
    "        def forward(self, x, lengths=None):\n",
    "            h_embedding = self.embedding(x.long())\n",
    "            h_embedding = self.embedding_dropout(h_embedding)\n",
    "\n",
    "            h_lstm1, _ = self.lstm1(h_embedding)\n",
    "            h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "\n",
    "            # global average pooling\n",
    "            avg_pool = torch.mean(h_lstm2, 1)\n",
    "            # global max pooling\n",
    "            max_pool, _ = torch.max(h_lstm2, 1)\n",
    "\n",
    "            h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "            h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "            h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "\n",
    "            hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "\n",
    "            result = self.linear_out(hidden)\n",
    "            aux_result = self.linear_aux_out(hidden)\n",
    "            out = torch.cat([result, aux_result], 1)\n",
    "\n",
    "            return out\n",
    "\n",
    "    def custom_loss(data, targets):\n",
    "        ''' Define custom loss function for weighted BCE on 'target' column '''\n",
    "        bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n",
    "        bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n",
    "        return (bce_loss_1 * loss_weight) + bce_loss_2\n",
    "\n",
    "    #Inference No training\n",
    "    #print(x_train_padded.shape)\n",
    "    print(x_test_padded.shape)\n",
    "    #print(y_train_torch.shape)\n",
    "    \n",
    "    rnn_results = []\n",
    "    for i in range(NUM_MODELS):\n",
    "        MODEL_DIR = paths[i]\n",
    "        \n",
    "        model_dict = {'fold0_models': ['0_model1_0_epoch.bin','0_model1_1_epoch.bin','0_model1_2_epoch.bin','0_model1_3_epoch.bin'], \n",
    "              'fold1_models': ['1_model1_0_epoch.bin','1_model1_1_epoch.bin','1_model1_2_epoch.bin','1_model1_3_epoch.bin'],\n",
    "              'fold2_models': ['2_model1_0_epoch.bin','2_model1_1_epoch.bin','2_model1_2_epoch.bin','2_model1_3_epoch.bin'],\n",
    "              'fold3_models': ['3_model1_0_epoch.bin','3_model1_1_epoch.bin','3_model1_2_epoch.bin','3_model1_3_epoch.bin'],\n",
    "              'fold4_models': ['4_model1_0_epoch.bin','4_model1_1_epoch.bin','4_model1_2_epoch.bin','4_model1_3_epoch.bin'] }\n",
    "\n",
    "        SPLITS = 5\n",
    "        EPOCHS = 4\n",
    "        OUTPUT_DIM = 7\n",
    "        batch_size = 512\n",
    "\n",
    "        test_dataset = data.TensorDataset(x_test_padded, test_lengths)\n",
    "        test_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n",
    "        test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "        checkpoint_weights = [2 ** epoch for epoch in range(EPOCHS)]\n",
    "\n",
    "        test_preds_all_folds = []\n",
    "        \n",
    "        #average predictions over 4 epochs per fold\n",
    "        for fld in model_dict.keys():    \n",
    "            test_preds = np.zeros((len(test_dataset),OUTPUT_DIM))\n",
    "            test_preds_all_epochs = []\n",
    "            for e in range(EPOCHS):\n",
    "                #print(model_dict[fld][e])\n",
    "                dict_load = torch.load(MODEL_DIR + model_dict[fld][e] )\n",
    "                dict_load['embedding.weight'] = torch.from_numpy(embedding_matrix)\n",
    "                m2 = NeuralNet(embedding_matrix,OUTPUT_DIM-1).cuda()  \n",
    "                m2.load_state_dict(dict_load)\n",
    "                m2.to('cuda')\n",
    "                for param in m2.parameters():\n",
    "                    param.requires_grad = False\n",
    "                m2.eval()\n",
    "\n",
    "                for i, x_batch in enumerate(test_loader):\n",
    "                    pred = m2(x_batch[0].to('cuda'))\n",
    "                    #print(pred.shape)\n",
    "                    test_preds[i * batch_size:(i + 1) * batch_size] = sigmoid(pred.detach().cpu().squeeze().numpy()) #.argmax(axis=-1)\n",
    "\n",
    "                #print(test_preds.shape)\n",
    "                test_preds_all_epochs.append(test_preds)\n",
    "\n",
    "            test_preds_all_folds.append(np.average(test_preds_all_epochs, weights=checkpoint_weights, axis=0))\n",
    "\n",
    "        test_pred_lstm = np.mean(test_preds_all_folds, axis=0)[:, 0]\n",
    "        rnn_results.append(test_pred_lstm) \n",
    "    \n",
    "    #delete everything except rnn_results\n",
    "    del train,test, symbols_to_isolate, symbols_to_delete,  y_aux_train, \n",
    "    del subgroup_bool_train, subgroup_negative_mask, toxic_bool_train, weights, loss_weight, \n",
    "    del embedding_matrix,  x_test_padded\n",
    "    #y_train_torch, x_train_padded, y_train,\n",
    "    del m2, test_dataset, test_collator, test_loader, test_preds_all_folds, test_pred_lstm, test_preds_all_epochs, \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return rnn_results #np.mean(rnn_results,axis=0)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = '../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'\n",
    "TEST_CSV = '../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv'\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)  \n",
    "test = pd.read_csv(TEST_CSV) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_model(paths, train, test):\n",
    "    \n",
    "    NUM_MODELS = len(paths)\n",
    "\n",
    "    GPT2_MODEL_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/'\n",
    "    TOKENIZER_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/'\n",
    "    GPT2_CONFIG_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/config.json'\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 220\n",
    "    SEED = 1234\n",
    "    BATCH_SIZE = 32\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    def convert_lines(example, max_seq_length,tokenizer):\n",
    "        max_seq_length -=2\n",
    "        all_tokens = []\n",
    "        longer = 0\n",
    "        #for text in tqdm_notebook(example):\n",
    "        for text in example:\n",
    "            tokens_a = tokenizer.tokenize(text)\n",
    "            if len(tokens_a)>max_seq_length:\n",
    "                tokens_a = tokens_a[:max_seq_length]\n",
    "                longer += 1\n",
    "            one_token = tokenizer.convert_tokens_to_ids(tokens_a)+[0] * (max_seq_length - len(tokens_a))\n",
    "            all_tokens.append(one_token)\n",
    "        print(longer)\n",
    "        return np.array(all_tokens)\n",
    "    \n",
    "    gpt2_config = GPT2Config(GPT2_CONFIG_PATH)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "    \n",
    "    test['comment_text'] = test['comment_text'].astype(str) \n",
    "    test['id1'] = np.arange(len(test))\n",
    "    \n",
    "    X_test = convert_lines(test[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "    y_columns = ['target', 'weights', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n",
    "    \n",
    "    gpt2_preds = []\n",
    "    for k in range(NUM_MODELS):\n",
    "        model = GPT2ClassificationHeadModel.from_pretrained(GPT2_MODEL_PATH, clf_dropout=0.2, n_class=len(y_columns))\n",
    "        model.load_state_dict(torch.load(paths[k]))\n",
    "        model.to(device)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.eval()\n",
    "    \n",
    "        test_preds = np.zeros((len(X_test)))\n",
    "        \n",
    "        \n",
    "        test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long),\n",
    "                                                  torch.tensor(test['id1'],dtype=torch.long))\n",
    "\n",
    "        ran_sampler = torch.utils.data.RandomSampler(test_dataset)\n",
    "        len_sampler = LenMatchBatchSampler(ran_sampler, batch_size=32, drop_last=False)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset,batch_sampler=len_sampler)\n",
    "\n",
    "        #test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n",
    "        #test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "        #tk0 = tqdm(test_loader)\n",
    "        tk0 = test_loader\n",
    "        for i, x_batch in enumerate(tk0):\n",
    "            tsrs = trim_tensors(x_batch)\n",
    "            b_input_ids, ids  = tuple(t.to(device) for t in tsrs)\n",
    "            pred = model(b_input_ids.to(device))\n",
    "            #test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "            test_preds[ids.detach().cpu().numpy()] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "    \n",
    "        test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n",
    "        \n",
    "        gpt2_preds.append(test_pred)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    del tokenizer, test, X_test\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return gpt2_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gpt2_model(paths, train, test):\n",
    "#     NUM_MODELS = len(paths)\n",
    "\n",
    "#     GPT2_MODEL_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/'\n",
    "#     TOKENIZER_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/'\n",
    "#     GPT2_CONFIG_PATH = '../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/config.json'\n",
    "\n",
    "#     MAX_SEQUENCE_LENGTH = 220\n",
    "#     SEED = 1234\n",
    "#     BATCH_SIZE = 32\n",
    "#     np.random.seed(SEED)\n",
    "#     torch.manual_seed(SEED)\n",
    "#     torch.cuda.manual_seed(SEED)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "#     def convert_lines(example, max_seq_length,tokenizer):\n",
    "#         max_seq_length -=2\n",
    "#         all_tokens = []\n",
    "#         longer = 0\n",
    "#         for text in tqdm_notebook(example):\n",
    "#             tokens_a = tokenizer.tokenize(text)\n",
    "#             if len(tokens_a)>max_seq_length:\n",
    "#                 tokens_a = tokens_a[:max_seq_length]\n",
    "#                 longer += 1\n",
    "#             one_token = tokenizer.convert_tokens_to_ids(tokens_a)+[0] * (max_seq_length - len(tokens_a))\n",
    "#             all_tokens.append(one_token)\n",
    "#         print(longer)\n",
    "#         return np.array(all_tokens)\n",
    "    \n",
    "#     gpt2_config = GPT2Config(GPT2_CONFIG_PATH)\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "    \n",
    "#     test['comment_text'] = test['comment_text'].astype(str) \n",
    "#     X_test = convert_lines(test[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "#     y_columns = ['target', 'weights', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n",
    "\n",
    "#     gpt2_preds = []\n",
    "#     for i in range(NUM_MODELS):\n",
    "#         model = GPT2ClassificationHeadModel.from_pretrained(GPT2_MODEL_PATH, clf_dropout=0.2, n_class=len(y_columns))\n",
    "#         model.load_state_dict(torch.load(paths[i]))\n",
    "#         model.to(device)\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         model.eval()\n",
    "    \n",
    "#         test_preds = np.zeros((len(X_test)))\n",
    "#         test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n",
    "#         test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "#         tk0 = tqdm(test_loader)\n",
    "#         for i, (x_batch,) in enumerate(tk0):\n",
    "#             pred = model(x_batch.to(device))\n",
    "#             test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "\n",
    "#         test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n",
    "        \n",
    "#         gpt2_preds.append(test_pred)\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "#     del tokenizer, test, X_test\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     return gpt2_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BERT Base...\n",
      "End BERT Base, time taken (seconds):  3386.538088321686\n"
     ]
    }
   ],
   "source": [
    "# ########################### Bert Large ###########################\n",
    "# BERT_LARGE_MODEL_DIR_BASE = '../input/tf-large-bert-inference-2/1560187254-20190610t173847z-001'\n",
    "# BERT_LARGE_MODEL = '/1560187254/'\n",
    "# start = time.time()\n",
    "# print(\"Starting BERT large...\")\n",
    "# bert_large_preds = bert_large(BERT_LARGE_MODEL_DIR_BASE, BERT_LARGE_MODEL, test)\n",
    "# del bert_large\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# #following required for bert large - GPU memory doesn't get released.\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "# print(\"End BERT large, time taken (seconds): \", time.time()-start)\n",
    "# ########################### Bert Large ###########################\n",
    "\n",
    "# # ########################### Bert Base ###########################\n",
    "BERT_BASE_MODEL_PATHS = ['../input/pytorch-bert-base-inference/bert_pytorch_mymodel.bin']  \n",
    "\n",
    "BERT_BASE_MODEL_PATHS = ['../input/bert-base-5-models/bert_model5folds/bert_model5folds/bert_pytorch_mymodel_fold1.bin',\n",
    "                        '../input/bert-base-5-models/bert_model5folds/bert_model5folds/bert_pytorch_mymodel_fold2.bin',\n",
    "                        '../input/bert-base-5-models/bert_model5folds/bert_model5folds/bert_pytorch_mymodel_fold3.bin',\n",
    "                        '../input/bert-base-5-models/bert_model5folds/bert_model5folds/bert_pytorch_mymodel_fold4.bin',\n",
    "                        '../input/bert-base-5-models/bert_model5folds/bert_model5folds/bert_pytorch_mymodel_fold5.bin',\n",
    "                        \n",
    "                        '../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold0.bin',\n",
    "                        '../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold1.bin',                         \n",
    "                        '../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold2.bin',\n",
    "                        '../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold3.bin',                         \n",
    "                        '../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold4.bin'                         \n",
    "                        ]\n",
    "\n",
    "\n",
    "#BERT_BASE_MODEL_PATHS = ['../input/2nd-bert-base-5-models/bert_pytorch_mymodel_fold1.bin']\n",
    "\n",
    "start = time.time()\n",
    "print(\"Starting BERT Base...\")\n",
    "bert_base_preds = bert_base(BERT_BASE_MODEL_PATHS,train,test)\n",
    "del bert_base\n",
    "gc.collect()\n",
    "print(\"End BERT Base, time taken (seconds): \", time.time()-start)\n",
    "# ########################### Bert Base ###########################\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_base_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPT2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../input/jigsawdatasets/pytorch-pretrained-gpt2/pytorch-pretrained-gpt2/pytorch_pretrained_bert/tokenization_gpt2.py:146: ResourceWarning: unclosed file <_io.TextIOWrapper name='../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/vocab.json' mode='r' encoding='UTF-8'>\n",
      "  self.encoder = json.load(open(vocab_file))\n",
      "../input/jigsawdatasets/pytorch-pretrained-gpt2/pytorch-pretrained-gpt2/pytorch_pretrained_bert/tokenization_gpt2.py:151: ResourceWarning: unclosed file <_io.TextIOWrapper name='../input/jigsawdatasets/gpt2-models_kaggle/gpt2-models_kaggle/merges.txt' mode='r' encoding='utf-8'>\n",
      "  bpe_data = open(merges_file, encoding='utf-8').read().split('\\n')[1:-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n",
      "End GPT2, time taken (seconds):  1771.2205953598022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################## GPT2  ###########################\n",
    "GPT2_MODEL_PATHS = ['../input/gpt2-models-for-ensemble/gpt2_pytorch_mymodel_fold0.bin',\n",
    "                   '../input/gpt2-models-for-ensemble/gpt2_pytorch_mymodel_fold1.bin',\n",
    "                   '../input/gpt2-models-for-ensemble/gpt2_pytorch_mymodel_fold2.bin',\n",
    "                   '../input/gpt2-models-for-ensemble/gpt2_pytorch_mymodel_fold3.bin',\n",
    "                   '../input/gpt2-models-for-ensemble/gpt2_pytorch_mymodel_fold4.bin']\n",
    "start = time.time()\n",
    "print(\"Starting GPT2...\")\n",
    "gpt2_preds = gpt2_model(GPT2_MODEL_PATHS, train, test)\n",
    "del gpt2_model\n",
    "gc.collect()\n",
    "print(\"End GPT2, time taken (seconds): \", time.time()-start)\n",
    "########################## GPT2  ###########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488836\n",
      "n unknown words (crawl):  148789\n",
      "n unknown words (glove):  152310\n",
      "torch.Size([97320, 300])\n",
      "End RNN, time taken (seconds):  1295.441802740097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ########################### LSTM  ###########################\n",
    "RNN_PATHS = ['../input/jigsaw-model3/', '../input/rnn-for-ensemble/rnn/']\n",
    "start = time.time()\n",
    "print(\"Starting RNN...\")\n",
    "rnn_preds = rnn_lstm(RNN_PATHS, train, test)\n",
    "del rnn_lstm\n",
    "gc.collect()\n",
    "print(\"End RNN, time taken (seconds): \", time.time()-start)\n",
    "########################### LSTM  ###########################\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = (0.35*np.mean(bert_base_preds,axis=0)  + 0.35*bert_large_preds[:,0]  + 0.3*np.mean(rnn_preds,axis=0))\n",
    "\n",
    "#predictions = (np.mean(bert_base_preds,axis=0)) #+ np.mean(rnn_preds,axis=0))/2 #np.mean(gpt2_preds,axis=0)\n",
    "\n",
    "# predictions = np.mean(gpt2_preds,axis=0) *23/101\n",
    "# predictions += np.mean(bert_base_preds,axis=0)*48/101 \n",
    "# predictions +=  np.mean(rnn_preds,axis=0)  * 30/101\n",
    "\n",
    "#predictions = (np.mean(bert_base_preds,axis=0))\n",
    "\n",
    "#28 bert + 27 bert + 16 rnn + 19 gpt2 + 11 rnn) \n",
    "predictions = np.mean(bert_base_preds[0:5],axis=0)*28/101.0 + np.mean(bert_base_preds[5:],axis=0)*27/101.0\n",
    "predictions += rnn_preds[0]*16/101.0 + rnn_preds[1]*11/101.0\n",
    "predictions += np.mean(gpt2_preds,axis=0)*19/101.0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction':  predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.002139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.002047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.000771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.989459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7000005</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7000006</td>\n",
       "      <td>0.001023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7000007</td>\n",
       "      <td>0.002543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7000008</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7000009</td>\n",
       "      <td>0.006536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7000010</td>\n",
       "      <td>0.000760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7000011</td>\n",
       "      <td>0.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7000012</td>\n",
       "      <td>0.000158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7000013</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7000014</td>\n",
       "      <td>0.000568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7000015</td>\n",
       "      <td>0.001716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7000016</td>\n",
       "      <td>0.037136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7000017</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7000018</td>\n",
       "      <td>0.260599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7000019</td>\n",
       "      <td>0.013342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7000020</td>\n",
       "      <td>0.007234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7000021</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7000022</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7000023</td>\n",
       "      <td>0.481425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7000024</td>\n",
       "      <td>0.920243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7000025</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7000026</td>\n",
       "      <td>0.046282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7000027</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7000028</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7000029</td>\n",
       "      <td>0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97290</th>\n",
       "      <td>7097290</td>\n",
       "      <td>0.001858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97291</th>\n",
       "      <td>7097291</td>\n",
       "      <td>0.338689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97292</th>\n",
       "      <td>7097292</td>\n",
       "      <td>0.125660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97293</th>\n",
       "      <td>7097293</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97294</th>\n",
       "      <td>7097294</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97295</th>\n",
       "      <td>7097295</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97296</th>\n",
       "      <td>7097296</td>\n",
       "      <td>0.001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97297</th>\n",
       "      <td>7097297</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97298</th>\n",
       "      <td>7097298</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97299</th>\n",
       "      <td>7097299</td>\n",
       "      <td>0.011112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97300</th>\n",
       "      <td>7097300</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97301</th>\n",
       "      <td>7097301</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97302</th>\n",
       "      <td>7097302</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97303</th>\n",
       "      <td>7097303</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97304</th>\n",
       "      <td>7097304</td>\n",
       "      <td>0.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97305</th>\n",
       "      <td>7097305</td>\n",
       "      <td>0.057545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97306</th>\n",
       "      <td>7097306</td>\n",
       "      <td>0.000444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97307</th>\n",
       "      <td>7097307</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97308</th>\n",
       "      <td>7097308</td>\n",
       "      <td>0.006310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97309</th>\n",
       "      <td>7097309</td>\n",
       "      <td>0.002865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97310</th>\n",
       "      <td>7097310</td>\n",
       "      <td>0.005392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97311</th>\n",
       "      <td>7097311</td>\n",
       "      <td>0.004360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97312</th>\n",
       "      <td>7097312</td>\n",
       "      <td>0.107040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97313</th>\n",
       "      <td>7097313</td>\n",
       "      <td>0.051441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97314</th>\n",
       "      <td>7097314</td>\n",
       "      <td>0.000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97315</th>\n",
       "      <td>7097315</td>\n",
       "      <td>0.002659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97316</th>\n",
       "      <td>7097316</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97317</th>\n",
       "      <td>7097317</td>\n",
       "      <td>0.016423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97318</th>\n",
       "      <td>7097318</td>\n",
       "      <td>0.050484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97319</th>\n",
       "      <td>7097319</td>\n",
       "      <td>0.000127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97320 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prediction\n",
       "0      7000000    0.002139\n",
       "1      7000001    0.000075\n",
       "2      7000002    0.002047\n",
       "3      7000003    0.000771\n",
       "4      7000004    0.989459\n",
       "5      7000005    0.000083\n",
       "6      7000006    0.001023\n",
       "7      7000007    0.002543\n",
       "8      7000008    0.016949\n",
       "9      7000009    0.006536\n",
       "10     7000010    0.000760\n",
       "11     7000011    0.093900\n",
       "12     7000012    0.000158\n",
       "13     7000013    0.000130\n",
       "14     7000014    0.000568\n",
       "15     7000015    0.001716\n",
       "16     7000016    0.037136\n",
       "17     7000017    0.000242\n",
       "18     7000018    0.260599\n",
       "19     7000019    0.013342\n",
       "20     7000020    0.007234\n",
       "21     7000021    0.000253\n",
       "22     7000022    0.000093\n",
       "23     7000023    0.481425\n",
       "24     7000024    0.920243\n",
       "25     7000025    0.000336\n",
       "26     7000026    0.046282\n",
       "27     7000027    0.000137\n",
       "28     7000028    0.000210\n",
       "29     7000029    0.001345\n",
       "...        ...         ...\n",
       "97290  7097290    0.001858\n",
       "97291  7097291    0.338689\n",
       "97292  7097292    0.125660\n",
       "97293  7097293    0.000074\n",
       "97294  7097294    0.000188\n",
       "97295  7097295    0.000193\n",
       "97296  7097296    0.001025\n",
       "97297  7097297    0.000177\n",
       "97298  7097298    0.000475\n",
       "97299  7097299    0.011112\n",
       "97300  7097300    0.000193\n",
       "97301  7097301    0.001572\n",
       "97302  7097302    0.000107\n",
       "97303  7097303    0.000175\n",
       "97304  7097304    0.108400\n",
       "97305  7097305    0.057545\n",
       "97306  7097306    0.000444\n",
       "97307  7097307    0.000109\n",
       "97308  7097308    0.006310\n",
       "97309  7097309    0.002865\n",
       "97310  7097310    0.005392\n",
       "97311  7097311    0.004360\n",
       "97312  7097312    0.107040\n",
       "97313  7097313    0.051441\n",
       "97314  7097314    0.000724\n",
       "97315  7097315    0.002659\n",
       "97316  7097316    0.000071\n",
       "97317  7097317    0.016423\n",
       "97318  7097318    0.050484\n",
       "97319  7097319    0.000127\n",
       "\n",
       "[97320 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame([np.mean(bert_base_preds,axis=0),np.mean(rnn_preds,axis=0),np.mean(gpt2_preds,axis=0)]).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
